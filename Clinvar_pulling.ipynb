{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install Bio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5tAi6ZinwBD",
        "outputId": "6819bb17-8245-4782-fa63-b55ccb6ca7ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Bio in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
            "Requirement already satisfied: biopython>=1.80 in /usr/local/lib/python3.11/dist-packages (from Bio) (1.85)\n",
            "Requirement already satisfied: gprofiler-official in /usr/local/lib/python3.11/dist-packages (from Bio) (1.0.0)\n",
            "Requirement already satisfied: mygene in /usr/local/lib/python3.11/dist-packages (from Bio) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from Bio) (2.2.2)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.11/dist-packages (from Bio) (1.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from Bio) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from Bio) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython>=1.80->Bio) (2.0.2)\n",
            "Requirement already satisfied: biothings-client>=0.2.6 in /usr/local/lib/python3.11/dist-packages (from mygene->Bio) (0.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->Bio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->Bio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->Bio) (2025.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch->Bio) (4.3.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pooch->Bio) (25.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->Bio) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->Bio) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->Bio) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->Bio) (2025.8.3)\n",
            "Requirement already satisfied: httpx>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from biothings-client>=0.2.6->mygene->Bio) (0.28.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->Bio) (1.17.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene->Bio) (4.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chMaHsxinayF",
        "outputId": "5d6b58a2-3b0f-4407-d723-ea213c3be39b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " CONFIGURATION:\n",
            "  Variants to process: ALL (~15,000)\n",
            "  Parallel workers: 15\n",
            "  Require citations: False\n",
            "  Include conflicts between labs: False\n",
            "  Show statistics: True\n",
            "================================================================================\n",
            "[INIT] Parallel ClinVar API initialized\n",
            "[INIT] Email: yl8889@nyu.edu\n",
            "[INIT] API Key: Configured\n",
            "[INIT] Rate limit: 10 requests/second\n",
            "\n",
            " Generating statistics from 1000 variants...\n",
            "\n",
            "[STATS] Analyzing first 1000 variants for statistics...\n",
            "\n",
            "================================================================================\n",
            "SEARCHING FOR ALL BRCA1 VARIANTS IN CLINVAR\n",
            "================================================================================\n",
            "[SEARCH] Found 15452 BRCA1 records in ClinVar\n",
            "[SEARCH] Retrieved 500/1000 IDs\n",
            "[SEARCH] Retrieved 1000/1000 IDs\n",
            "\n",
            "[PARALLEL] Processing 1000 variants with 10 workers\n",
            "[PARALLEL] Progress: 100/1000 (10.0%) - Rate: 8.1 variants/sec - ETA: 1.9 minutes\n",
            "[PARALLEL] Progress: 200/1000 (20.0%) - Rate: 8.5 variants/sec - ETA: 1.6 minutes\n",
            "[PARALLEL] Progress: 300/1000 (30.0%) - Rate: 8.5 variants/sec - ETA: 1.4 minutes\n",
            "[PARALLEL] Progress: 400/1000 (40.0%) - Rate: 8.6 variants/sec - ETA: 1.2 minutes\n",
            "[PARALLEL] Progress: 500/1000 (50.0%) - Rate: 8.7 variants/sec - ETA: 1.0 minutes\n",
            "[PARALLEL] Progress: 600/1000 (60.0%) - Rate: 8.6 variants/sec - ETA: 0.8 minutes\n",
            "[PARALLEL] Progress: 700/1000 (70.0%) - Rate: 8.6 variants/sec - ETA: 0.6 minutes\n",
            "[PARALLEL] Progress: 800/1000 (80.0%) - Rate: 8.7 variants/sec - ETA: 0.4 minutes\n",
            "[PARALLEL] Progress: 900/1000 (90.0%) - Rate: 8.7 variants/sec - ETA: 0.2 minutes\n",
            "[PARALLEL] Progress: 1000/1000 (100.0%) - Rate: 8.6 variants/sec - ETA: 0.0 minutes\n",
            "[PARALLEL] Completed in 116.0 seconds (1.9 minutes)\n",
            "[PARALLEL] Successfully processed: 997\n",
            "[PARALLEL] Failed: 3\n",
            "[PARALLEL] Average rate: 8.6 variants/second\n",
            "\n",
            " BRCA1 VARIANT STATISTICS (based on 1000 variants):\n",
            "  Variants analyzed: 997\n",
            "  Variants with multiple submissions: 63\n",
            "  Variants with changes: 8\n",
            "  Variants with changes + citations: 0\n",
            "\n",
            "  Expected yields:\n",
            "    With citations filter: ~0.00% of variants\n",
            "    Without citations filter: ~0.80% of variants\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            " Starting processing...\n",
            "  Processing ALL variants\n",
            "  Using 15 parallel workers\n",
            "  Citation filter: DISABLED (all changes)\n",
            "  Conflict detection: DISABLED (temporal changes only)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "TRULY PARALLEL PROCESSING - ALL BRCA1 VARIANTS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "SEARCHING FOR ALL BRCA1 VARIANTS IN CLINVAR\n",
            "================================================================================\n",
            "[SEARCH] Found 15452 BRCA1 records in ClinVar\n",
            "[SEARCH] Retrieved 500/15452 IDs\n",
            "[SEARCH] Retrieved 1000/15452 IDs\n",
            "[SEARCH] Retrieved 1500/15452 IDs\n",
            "[SEARCH] Retrieved 2000/15452 IDs\n",
            "[SEARCH] Retrieved 2500/15452 IDs\n",
            "[SEARCH] Retrieved 3000/15452 IDs\n",
            "[SEARCH] Retrieved 3500/15452 IDs\n",
            "[SEARCH] Retrieved 4000/15452 IDs\n",
            "[SEARCH] Retrieved 4500/15452 IDs\n",
            "[SEARCH] Retrieved 5000/15452 IDs\n",
            "[SEARCH] Retrieved 5500/15452 IDs\n",
            "[SEARCH] Retrieved 6000/15452 IDs\n",
            "[SEARCH] Retrieved 6500/15452 IDs\n",
            "[SEARCH] Retrieved 7000/15452 IDs\n",
            "[SEARCH] Retrieved 7500/15452 IDs\n",
            "[SEARCH] Retrieved 8000/15452 IDs\n",
            "[SEARCH] Retrieved 8500/15452 IDs\n",
            "[SEARCH] Retrieved 9000/15452 IDs\n",
            "[SEARCH] Retrieved 9500/15452 IDs\n",
            "[SEARCH] Retrieved 10000/15452 IDs\n",
            "[SEARCH] Retrieved 10500/15452 IDs\n",
            "[SEARCH] Retrieved 11000/15452 IDs\n",
            "[SEARCH] Retrieved 11500/15452 IDs\n",
            "[SEARCH] Retrieved 12000/15452 IDs\n",
            "[SEARCH] Retrieved 12500/15452 IDs\n",
            "[SEARCH] Retrieved 13000/15452 IDs\n",
            "[SEARCH] Retrieved 13500/15452 IDs\n",
            "[SEARCH] Retrieved 14000/15452 IDs\n",
            "[SEARCH] Retrieved 14500/15452 IDs\n",
            "[SEARCH] Retrieved 15000/15452 IDs\n",
            "[SEARCH] Retrieved 15452/15452 IDs\n",
            "[CACHE] Saved complete variant list\n",
            "\n",
            "[PROCESS] Processing 15452 variants...\n",
            "[OPTIMIZE] Using 15 parallel workers for API fetching AND parsing\n",
            "[FILTER] Require citations: False\n",
            "[FILTER] Include conflicts: False\n",
            "\n",
            "[PARALLEL] Processing 15452 variants with 15 workers\n",
            "[PARALLEL] Progress: 100/15452 (0.6%) - Rate: 122.6 variants/sec - ETA: 2.1 minutes\n",
            "[PARALLEL] Progress: 200/15452 (1.3%) - Rate: 245.0 variants/sec - ETA: 1.0 minutes\n",
            "[PARALLEL] Progress: 300/15452 (1.9%) - Rate: 367.4 variants/sec - ETA: 0.7 minutes\n",
            "[PARALLEL] Progress: 400/15452 (2.6%) - Rate: 489.6 variants/sec - ETA: 0.5 minutes\n",
            "[PARALLEL] Progress: 500/15452 (3.2%) - Rate: 611.8 variants/sec - ETA: 0.4 minutes\n",
            "[PARALLEL] Progress: 600/15452 (3.9%) - Rate: 734.0 variants/sec - ETA: 0.3 minutes\n",
            "[PARALLEL] Progress: 700/15452 (4.5%) - Rate: 856.0 variants/sec - ETA: 0.3 minutes\n",
            "[PARALLEL] Progress: 800/15452 (5.2%) - Rate: 977.9 variants/sec - ETA: 0.2 minutes\n",
            "[PARALLEL] Progress: 900/15452 (5.8%) - Rate: 1099.8 variants/sec - ETA: 0.2 minutes\n",
            "[PARALLEL] Progress: 1000/15452 (6.5%) - Rate: 1204.0 variants/sec - ETA: 0.2 minutes\n",
            "[PARALLEL] Progress: 1100/15452 (7.1%) - Rate: 84.6 variants/sec - ETA: 2.8 minutes\n",
            "[PARALLEL] Progress: 1200/15452 (7.8%) - Rate: 48.4 variants/sec - ETA: 4.9 minutes\n",
            "[PARALLEL] Progress: 1300/15452 (8.4%) - Rate: 35.0 variants/sec - ETA: 6.7 minutes\n",
            "[PARALLEL] Progress: 1400/15452 (9.1%) - Rate: 28.6 variants/sec - ETA: 8.2 minutes\n",
            "[PARALLEL] Progress: 1500/15452 (9.7%) - Rate: 24.8 variants/sec - ETA: 9.4 minutes\n",
            "[PARALLEL] Progress: 1600/15452 (10.4%) - Rate: 22.2 variants/sec - ETA: 10.4 minutes\n",
            "[PARALLEL] Progress: 1700/15452 (11.0%) - Rate: 20.2 variants/sec - ETA: 11.3 minutes\n",
            "[PARALLEL] Progress: 1800/15452 (11.6%) - Rate: 18.9 variants/sec - ETA: 12.0 minutes\n",
            "[PARALLEL] Progress: 1900/15452 (12.3%) - Rate: 17.9 variants/sec - ETA: 12.6 minutes\n",
            "[PARALLEL] Progress: 2000/15452 (12.9%) - Rate: 17.0 variants/sec - ETA: 13.2 minutes\n",
            "[PARALLEL] Progress: 2100/15452 (13.6%) - Rate: 16.3 variants/sec - ETA: 13.7 minutes\n",
            "[PARALLEL] Progress: 2200/15452 (14.2%) - Rate: 15.6 variants/sec - ETA: 14.1 minutes\n",
            "[PARALLEL] Progress: 2300/15452 (14.9%) - Rate: 15.1 variants/sec - ETA: 14.5 minutes\n",
            "[PARALLEL] Progress: 2400/15452 (15.5%) - Rate: 14.7 variants/sec - ETA: 14.8 minutes\n",
            "[PARALLEL] Progress: 2500/15452 (16.2%) - Rate: 14.3 variants/sec - ETA: 15.1 minutes\n",
            "[PARALLEL] Progress: 2600/15452 (16.8%) - Rate: 13.9 variants/sec - ETA: 15.4 minutes\n",
            "[PARALLEL] Progress: 2700/15452 (17.5%) - Rate: 13.6 variants/sec - ETA: 15.6 minutes\n",
            "[PARALLEL] Progress: 2800/15452 (18.1%) - Rate: 13.4 variants/sec - ETA: 15.8 minutes\n",
            "[PARALLEL] Progress: 2900/15452 (18.8%) - Rate: 13.1 variants/sec - ETA: 15.9 minutes\n",
            "[PARALLEL] Progress: 3000/15452 (19.4%) - Rate: 12.9 variants/sec - ETA: 16.1 minutes\n",
            "[PARALLEL] Progress: 3100/15452 (20.1%) - Rate: 12.7 variants/sec - ETA: 16.2 minutes\n",
            "[PARALLEL] Progress: 3200/15452 (20.7%) - Rate: 12.5 variants/sec - ETA: 16.3 minutes\n",
            "[PARALLEL] Progress: 3300/15452 (21.4%) - Rate: 12.4 variants/sec - ETA: 16.4 minutes\n",
            "[PARALLEL] Progress: 3400/15452 (22.0%) - Rate: 12.2 variants/sec - ETA: 16.4 minutes\n",
            "[PARALLEL] Progress: 3500/15452 (22.7%) - Rate: 12.1 variants/sec - ETA: 16.5 minutes\n",
            "[PARALLEL] Progress: 3600/15452 (23.3%) - Rate: 11.9 variants/sec - ETA: 16.5 minutes\n",
            "[PARALLEL] Progress: 3700/15452 (23.9%) - Rate: 11.8 variants/sec - ETA: 16.6 minutes\n",
            "[PARALLEL] Progress: 3800/15452 (24.6%) - Rate: 11.7 variants/sec - ETA: 16.6 minutes\n",
            "[PARALLEL] Progress: 3900/15452 (25.2%) - Rate: 11.6 variants/sec - ETA: 16.6 minutes\n",
            "[PARALLEL] Progress: 4000/15452 (25.9%) - Rate: 11.5 variants/sec - ETA: 16.6 minutes\n",
            "[PARALLEL] Progress: 4100/15452 (26.5%) - Rate: 11.4 variants/sec - ETA: 16.6 minutes\n",
            "[PARALLEL] Progress: 4200/15452 (27.2%) - Rate: 11.3 variants/sec - ETA: 16.5 minutes\n",
            "[PARALLEL] Progress: 4300/15452 (27.8%) - Rate: 11.3 variants/sec - ETA: 16.5 minutes\n",
            "[PARALLEL] Progress: 4400/15452 (28.5%) - Rate: 11.2 variants/sec - ETA: 16.4 minutes\n",
            "[PARALLEL] Progress: 4500/15452 (29.1%) - Rate: 11.1 variants/sec - ETA: 16.4 minutes\n",
            "[PARALLEL] Progress: 4600/15452 (29.8%) - Rate: 11.1 variants/sec - ETA: 16.4 minutes\n",
            "[PARALLEL] Progress: 4700/15452 (30.4%) - Rate: 11.0 variants/sec - ETA: 16.3 minutes\n",
            "[PARALLEL] Progress: 4800/15452 (31.1%) - Rate: 10.9 variants/sec - ETA: 16.3 minutes\n",
            "[PARALLEL] Progress: 4900/15452 (31.7%) - Rate: 10.9 variants/sec - ETA: 16.2 minutes\n",
            "[PARALLEL] Progress: 5000/15452 (32.4%) - Rate: 10.8 variants/sec - ETA: 16.1 minutes\n",
            "[PARALLEL] Progress: 5100/15452 (33.0%) - Rate: 10.7 variants/sec - ETA: 16.1 minutes\n",
            "[PARALLEL] Progress: 5200/15452 (33.7%) - Rate: 10.7 variants/sec - ETA: 16.0 minutes\n",
            "[PARALLEL] Progress: 5300/15452 (34.3%) - Rate: 10.7 variants/sec - ETA: 15.9 minutes\n",
            "[PARALLEL] Progress: 5400/15452 (34.9%) - Rate: 10.6 variants/sec - ETA: 15.8 minutes\n",
            "[PARALLEL] Progress: 5500/15452 (35.6%) - Rate: 10.6 variants/sec - ETA: 15.7 minutes\n",
            "[PARALLEL] Progress: 5600/15452 (36.2%) - Rate: 10.5 variants/sec - ETA: 15.6 minutes\n",
            "[PARALLEL] Progress: 5700/15452 (36.9%) - Rate: 10.4 variants/sec - ETA: 15.6 minutes\n",
            "[PARALLEL] Progress: 5800/15452 (37.5%) - Rate: 10.4 variants/sec - ETA: 15.5 minutes\n",
            "[PARALLEL] Progress: 5900/15452 (38.2%) - Rate: 10.4 variants/sec - ETA: 15.3 minutes\n",
            "[PARALLEL] Progress: 6000/15452 (38.8%) - Rate: 10.3 variants/sec - ETA: 15.2 minutes\n",
            "[PARALLEL] Progress: 6100/15452 (39.5%) - Rate: 10.3 variants/sec - ETA: 15.1 minutes\n",
            "[PARALLEL] Progress: 6200/15452 (40.1%) - Rate: 10.3 variants/sec - ETA: 15.0 minutes\n",
            "[PARALLEL] Progress: 6300/15452 (40.8%) - Rate: 10.2 variants/sec - ETA: 14.9 minutes\n",
            "[PARALLEL] Progress: 6400/15452 (41.4%) - Rate: 10.2 variants/sec - ETA: 14.8 minutes\n",
            "[PARALLEL] Progress: 6500/15452 (42.1%) - Rate: 10.2 variants/sec - ETA: 14.6 minutes\n",
            "[PARALLEL] Progress: 6600/15452 (42.7%) - Rate: 10.2 variants/sec - ETA: 14.5 minutes\n",
            "[PARALLEL] Progress: 6700/15452 (43.4%) - Rate: 10.1 variants/sec - ETA: 14.4 minutes\n",
            "[PARALLEL] Progress: 6800/15452 (44.0%) - Rate: 10.1 variants/sec - ETA: 14.3 minutes\n",
            "[PARALLEL] Progress: 6900/15452 (44.7%) - Rate: 10.1 variants/sec - ETA: 14.2 minutes\n",
            "[PARALLEL] Progress: 7000/15452 (45.3%) - Rate: 10.0 variants/sec - ETA: 14.0 minutes\n",
            "[PARALLEL] Progress: 7100/15452 (45.9%) - Rate: 10.0 variants/sec - ETA: 13.9 minutes\n",
            "[PARALLEL] Progress: 7200/15452 (46.6%) - Rate: 10.0 variants/sec - ETA: 13.8 minutes\n",
            "[PARALLEL] Progress: 7300/15452 (47.2%) - Rate: 10.0 variants/sec - ETA: 13.6 minutes\n",
            "[PARALLEL] Progress: 7400/15452 (47.9%) - Rate: 9.9 variants/sec - ETA: 13.5 minutes\n",
            "[PARALLEL] Progress: 7500/15452 (48.5%) - Rate: 9.9 variants/sec - ETA: 13.4 minutes\n",
            "[PARALLEL] Progress: 7600/15452 (49.2%) - Rate: 9.9 variants/sec - ETA: 13.2 minutes\n",
            "[PARALLEL] Progress: 7700/15452 (49.8%) - Rate: 9.9 variants/sec - ETA: 13.1 minutes\n",
            "[PARALLEL] Progress: 7800/15452 (50.5%) - Rate: 9.8 variants/sec - ETA: 12.9 minutes\n",
            "[PARALLEL] Progress: 7900/15452 (51.1%) - Rate: 9.8 variants/sec - ETA: 12.8 minutes\n",
            "[PARALLEL] Progress: 8000/15452 (51.8%) - Rate: 9.8 variants/sec - ETA: 12.7 minutes\n",
            "[PARALLEL] Progress: 8100/15452 (52.4%) - Rate: 9.8 variants/sec - ETA: 12.5 minutes\n",
            "[PARALLEL] Progress: 8200/15452 (53.1%) - Rate: 9.8 variants/sec - ETA: 12.3 minutes\n",
            "[PARALLEL] Progress: 8300/15452 (53.7%) - Rate: 9.8 variants/sec - ETA: 12.2 minutes\n",
            "[PARALLEL] Progress: 8400/15452 (54.4%) - Rate: 9.8 variants/sec - ETA: 12.0 minutes\n",
            "[PARALLEL] Progress: 8500/15452 (55.0%) - Rate: 9.7 variants/sec - ETA: 11.9 minutes\n",
            "[PARALLEL] Progress: 8600/15452 (55.7%) - Rate: 9.7 variants/sec - ETA: 11.7 minutes\n",
            "[PARALLEL] Progress: 8700/15452 (56.3%) - Rate: 9.7 variants/sec - ETA: 11.6 minutes\n",
            "[PARALLEL] Progress: 8800/15452 (57.0%) - Rate: 9.7 variants/sec - ETA: 11.4 minutes\n",
            "[PARALLEL] Progress: 8900/15452 (57.6%) - Rate: 9.7 variants/sec - ETA: 11.3 minutes\n",
            "[PARALLEL] Progress: 9000/15452 (58.2%) - Rate: 9.7 variants/sec - ETA: 11.1 minutes\n",
            "[PARALLEL] Progress: 9100/15452 (58.9%) - Rate: 9.7 variants/sec - ETA: 11.0 minutes\n",
            "[PARALLEL] Progress: 9200/15452 (59.5%) - Rate: 9.6 variants/sec - ETA: 10.8 minutes\n",
            "[PARALLEL] Progress: 9300/15452 (60.2%) - Rate: 9.6 variants/sec - ETA: 10.6 minutes\n",
            "[PARALLEL] Progress: 9400/15452 (60.8%) - Rate: 9.6 variants/sec - ETA: 10.5 minutes\n",
            "[PARALLEL] Progress: 9500/15452 (61.5%) - Rate: 9.6 variants/sec - ETA: 10.3 minutes\n",
            "[PARALLEL] Progress: 9600/15452 (62.1%) - Rate: 9.6 variants/sec - ETA: 10.2 minutes\n",
            "[PARALLEL] Progress: 9700/15452 (62.8%) - Rate: 9.6 variants/sec - ETA: 10.0 minutes\n",
            "[PARALLEL] Progress: 9800/15452 (63.4%) - Rate: 9.6 variants/sec - ETA: 9.8 minutes\n",
            "[PARALLEL] Progress: 9900/15452 (64.1%) - Rate: 9.6 variants/sec - ETA: 9.7 minutes\n",
            "[PARALLEL] Progress: 10000/15452 (64.7%) - Rate: 9.5 variants/sec - ETA: 9.5 minutes\n",
            "[PARALLEL] Progress: 10100/15452 (65.4%) - Rate: 9.5 variants/sec - ETA: 9.4 minutes\n",
            "[PARALLEL] Progress: 10200/15452 (66.0%) - Rate: 9.5 variants/sec - ETA: 9.2 minutes\n",
            "[PARALLEL] Progress: 10300/15452 (66.7%) - Rate: 9.5 variants/sec - ETA: 9.0 minutes\n",
            "[PARALLEL] Progress: 10400/15452 (67.3%) - Rate: 9.5 variants/sec - ETA: 8.9 minutes\n",
            "[PARALLEL] Progress: 10500/15452 (68.0%) - Rate: 9.5 variants/sec - ETA: 8.7 minutes\n",
            "[PARALLEL] Progress: 10600/15452 (68.6%) - Rate: 9.5 variants/sec - ETA: 8.5 minutes\n",
            "[PARALLEL] Progress: 10700/15452 (69.2%) - Rate: 9.5 variants/sec - ETA: 8.4 minutes\n",
            "[PARALLEL] Progress: 10800/15452 (69.9%) - Rate: 9.5 variants/sec - ETA: 8.2 minutes\n",
            "[PARALLEL] Progress: 10900/15452 (70.5%) - Rate: 9.5 variants/sec - ETA: 8.0 minutes\n",
            "[PARALLEL] Progress: 11000/15452 (71.2%) - Rate: 9.4 variants/sec - ETA: 7.9 minutes\n",
            "[PARALLEL] Progress: 11100/15452 (71.8%) - Rate: 9.4 variants/sec - ETA: 7.7 minutes\n",
            "[PARALLEL] Progress: 11200/15452 (72.5%) - Rate: 9.4 variants/sec - ETA: 7.5 minutes\n",
            "[PARALLEL] Progress: 11300/15452 (73.1%) - Rate: 9.4 variants/sec - ETA: 7.3 minutes\n",
            "[PARALLEL] Progress: 11400/15452 (73.8%) - Rate: 9.4 variants/sec - ETA: 7.2 minutes\n",
            "[PARALLEL] Progress: 11500/15452 (74.4%) - Rate: 9.4 variants/sec - ETA: 7.0 minutes\n",
            "[PARALLEL] Progress: 11600/15452 (75.1%) - Rate: 9.4 variants/sec - ETA: 6.8 minutes\n",
            "[PARALLEL] Progress: 11700/15452 (75.7%) - Rate: 9.4 variants/sec - ETA: 6.7 minutes\n",
            "[PARALLEL] Progress: 11800/15452 (76.4%) - Rate: 9.4 variants/sec - ETA: 6.5 minutes\n",
            "[PARALLEL] Progress: 11900/15452 (77.0%) - Rate: 9.4 variants/sec - ETA: 6.3 minutes\n",
            "[PARALLEL] Progress: 12000/15452 (77.7%) - Rate: 9.4 variants/sec - ETA: 6.2 minutes\n",
            "[PARALLEL] Progress: 12100/15452 (78.3%) - Rate: 9.3 variants/sec - ETA: 6.0 minutes\n",
            "[PARALLEL] Progress: 12200/15452 (79.0%) - Rate: 9.3 variants/sec - ETA: 5.8 minutes\n",
            "[PARALLEL] Progress: 12300/15452 (79.6%) - Rate: 9.3 variants/sec - ETA: 5.6 minutes\n",
            "[PARALLEL] Progress: 12400/15452 (80.2%) - Rate: 9.3 variants/sec - ETA: 5.5 minutes\n",
            "[PARALLEL] Progress: 12500/15452 (80.9%) - Rate: 9.3 variants/sec - ETA: 5.3 minutes\n",
            "[PARALLEL] Progress: 12600/15452 (81.5%) - Rate: 9.3 variants/sec - ETA: 5.1 minutes\n",
            "[PARALLEL] Progress: 12700/15452 (82.2%) - Rate: 9.3 variants/sec - ETA: 4.9 minutes\n",
            "[PARALLEL] Progress: 12800/15452 (82.8%) - Rate: 9.3 variants/sec - ETA: 4.8 minutes\n",
            "[PARALLEL] Progress: 12900/15452 (83.5%) - Rate: 9.3 variants/sec - ETA: 4.6 minutes\n",
            "[PARALLEL] Progress: 13000/15452 (84.1%) - Rate: 9.3 variants/sec - ETA: 4.4 minutes\n",
            "[PARALLEL] Progress: 13100/15452 (84.8%) - Rate: 9.3 variants/sec - ETA: 4.2 minutes\n",
            "[PARALLEL] Progress: 13200/15452 (85.4%) - Rate: 9.3 variants/sec - ETA: 4.1 minutes\n",
            "[PARALLEL] Progress: 13300/15452 (86.1%) - Rate: 9.3 variants/sec - ETA: 3.9 minutes\n",
            "[PARALLEL] Progress: 13400/15452 (86.7%) - Rate: 9.2 variants/sec - ETA: 3.7 minutes\n",
            "[PARALLEL] Progress: 13500/15452 (87.4%) - Rate: 9.2 variants/sec - ETA: 3.5 minutes\n",
            "[PARALLEL] Progress: 13600/15452 (88.0%) - Rate: 9.2 variants/sec - ETA: 3.3 minutes\n",
            "[PARALLEL] Progress: 13700/15452 (88.7%) - Rate: 9.2 variants/sec - ETA: 3.2 minutes\n",
            "[PARALLEL] Progress: 13800/15452 (89.3%) - Rate: 9.2 variants/sec - ETA: 3.0 minutes\n",
            "[PARALLEL] Progress: 13900/15452 (90.0%) - Rate: 9.2 variants/sec - ETA: 2.8 minutes\n",
            "[PARALLEL] Progress: 14000/15452 (90.6%) - Rate: 9.2 variants/sec - ETA: 2.6 minutes\n",
            "[PARALLEL] Progress: 14100/15452 (91.3%) - Rate: 9.2 variants/sec - ETA: 2.5 minutes\n",
            "[PARALLEL] Progress: 14200/15452 (91.9%) - Rate: 9.2 variants/sec - ETA: 2.3 minutes\n",
            "[PARALLEL] Progress: 14300/15452 (92.5%) - Rate: 9.2 variants/sec - ETA: 2.1 minutes\n",
            "[PARALLEL] Progress: 14400/15452 (93.2%) - Rate: 9.2 variants/sec - ETA: 1.9 minutes\n",
            "[PARALLEL] Progress: 14500/15452 (93.8%) - Rate: 9.2 variants/sec - ETA: 1.7 minutes\n",
            "[PARALLEL] Progress: 14600/15452 (94.5%) - Rate: 9.1 variants/sec - ETA: 1.6 minutes\n",
            "[PARALLEL] Progress: 14700/15452 (95.1%) - Rate: 9.1 variants/sec - ETA: 1.4 minutes\n",
            "[PARALLEL] Progress: 14800/15452 (95.8%) - Rate: 9.1 variants/sec - ETA: 1.2 minutes\n",
            "[PARALLEL] Progress: 14900/15452 (96.4%) - Rate: 9.1 variants/sec - ETA: 1.0 minutes\n",
            "[PARALLEL] Progress: 15000/15452 (97.1%) - Rate: 9.1 variants/sec - ETA: 0.8 minutes\n",
            "[PARALLEL] Progress: 15100/15452 (97.7%) - Rate: 9.1 variants/sec - ETA: 0.6 minutes\n",
            "[PARALLEL] Progress: 15200/15452 (98.4%) - Rate: 9.1 variants/sec - ETA: 0.5 minutes\n",
            "[PARALLEL] Progress: 15300/15452 (99.0%) - Rate: 9.1 variants/sec - ETA: 0.3 minutes\n",
            "[PARALLEL] Progress: 15400/15452 (99.7%) - Rate: 9.1 variants/sec - ETA: 0.1 minutes\n",
            "[PARALLEL] Completed in 1699.7 seconds (28.3 minutes)\n",
            "[PARALLEL] Successfully processed: 15397\n",
            "[PARALLEL] Failed: 55\n",
            "[PARALLEL] Average rate: 9.1 variants/second\n",
            "\n",
            "[SAVE] Results saved to: BRCA1_changes_parallel_20250817_152614.json\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS COMPLETE\n",
            "================================================================================\n",
            "\n",
            " PERFORMANCE:\n",
            "  Processing time: 1705.6 seconds\n",
            "  Processing time: 28.4 minutes\n",
            "  Variants processed: 15397\n",
            "  Processing rate: 9.0 variants/second\n",
            "  Parallel workers used: 15\n",
            "\n",
            " RESULTS:\n",
            "  Variants with changes: 17\n",
            "  Percentage with changes: 0.1%\n",
            "\n",
            "  Change type distribution:\n",
            "    other_change: 14\n",
            "    downgraded_to_benign: 2\n",
            "    upgraded_to_pathogenic: 1\n",
            "    upgraded_to_uncertain: 1\n",
            "\n",
            "  Sample classification changes (first 3):\n",
            "\n",
            "    Variant ID: 619018\n",
            "    Name: NM_007294.4(BRCA1):c.2283A>C (p.Glu761Asp)\n",
            "    RS: rs1567796302\n",
            "    Change by University of Washington Department of Laboratory Medicine, University of Washington:\n",
            "      Benign → Likely benign\n",
            "      (2018-03-28 to 2023-03-23)\n",
            "\n",
            "    Variant ID: 531444\n",
            "    Name: NM_007294.4(BRCA1):c.5056C>T (p.His1686Tyr)\n",
            "    RS: rs1555579648\n",
            "    Change by Fulgent Genetics, Fulgent Genetics:\n",
            "      Uncertain significance → Likely pathogenic\n",
            "      (2018-10-31 to 2024-02-08)\n",
            "\n",
            "    Variant ID: 264856\n",
            "    Name: NM_007294.4(BRCA1):c.*873del\n",
            "    RS: rs59541324\n",
            "    Change by Illumina Laboratory Services, Illumina:\n",
            "      Uncertain significance → Likely benign\n",
            "      (2016-06-14 to 2016-06-14)\n",
            "\n",
            "================================================================================\n",
            "PROCESSING COMPLETE\n",
            "================================================================================\n",
            "\n",
            " FINAL RESULTS:\n",
            "  Total time: 1705.7 seconds (28.4 minutes)\n",
            "  Variants processed: 15397\n",
            "  Variants with changes: 17\n",
            "  Processing rate: 9.0 variants/second\n",
            "\n",
            "  Change percentage: 0.11%\n",
            "\n",
            "  Total classification events: 18\n",
            "    Temporal changes (same lab): 18\n",
            "    Conflicts (between labs): 0\n",
            "\n",
            "  Change type distribution:\n",
            "    other_change: 14 (77.8%)\n",
            "    downgraded_to_benign: 2 (11.1%)\n",
            "    upgraded_to_pathogenic: 1 (5.6%)\n",
            "    upgraded_to_uncertain: 1 (5.6%)\n",
            "\n",
            " Results saved to: BRCA1_changes_parallel_20250817_152614.json\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "from datetime import datetime\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Set, Optional\n",
        "from collections import defaultdict\n",
        "import logging\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import gzip\n",
        "import pickle\n",
        "from functools import lru_cache\n",
        "import queue\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ClinVarParallelAPI:\n",
        "    def __init__(self, email: str, api_key: str = None, cache_dir: str = \"clinvar_cache_parallel\"):\n",
        "        self.email = email\n",
        "        self.api_key = api_key\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Create subdirectories for different cache types\n",
        "        self.xml_cache_dir = self.cache_dir / \"xml\"\n",
        "        self.xml_cache_dir.mkdir(exist_ok=True)\n",
        "        self.parsed_cache_dir = self.cache_dir / \"parsed\"\n",
        "        self.parsed_cache_dir.mkdir(exist_ok=True)\n",
        "        self.stats_cache = self.cache_dir / \"stats.json\"\n",
        "\n",
        "        self.eutils_base = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
        "\n",
        "        self.requests_per_second = 10 if api_key else 3\n",
        "        self.rate_limiter = threading.Semaphore(self.requests_per_second)\n",
        "        self.rate_limit_lock = threading.Lock()\n",
        "        self.last_request_times = []\n",
        "\n",
        "        print(f\"[INIT] Parallel ClinVar API initialized\")\n",
        "        print(f\"[INIT] Email: {self.email}\")\n",
        "        print(f\"[INIT] API Key: {'Configured' if self.api_key else 'Not provided'}\")\n",
        "        print(f\"[INIT] Rate limit: {self.requests_per_second} requests/second\")\n",
        "\n",
        "    def search_brca1_variants(self, limit: int = None) -> List[str]:\n",
        "        \"\"\"\n",
        "        Search for ALL BRCA1 variants in ClinVar\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"SEARCHING FOR ALL BRCA1 VARIANTS IN CLINVAR\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Check if cached variant list\n",
        "        variants_cache = self.cache_dir / \"brca1_variant_ids.json\"\n",
        "        if variants_cache.exists() and not limit:\n",
        "            with open(variants_cache, 'r') as f:\n",
        "                cached_ids = json.load(f)\n",
        "                print(f\"[CACHE] Using cached list of {len(cached_ids)} BRCA1 variants\")\n",
        "                return cached_ids\n",
        "\n",
        "        variation_ids = []\n",
        "\n",
        "        try:\n",
        "            search_url = f\"{self.eutils_base}/esearch.fcgi\"\n",
        "            search_query = 'BRCA1[gene]'\n",
        "\n",
        "            # Get count first\n",
        "            count_params = {\n",
        "                'db': 'clinvar',\n",
        "                'term': search_query,\n",
        "                'retmode': 'json',\n",
        "                'retmax': 0,\n",
        "                'email': self.email\n",
        "            }\n",
        "\n",
        "            if self.api_key:\n",
        "                count_params['api_key'] = self.api_key\n",
        "\n",
        "            response = requests.get(search_url, params=count_params, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            total_count = int(data.get('esearchresult', {}).get('count', 0))\n",
        "            print(f\"[SEARCH] Found {total_count} BRCA1 records in ClinVar\")\n",
        "\n",
        "            if limit:\n",
        "                retmax = min(limit, total_count)\n",
        "            else:\n",
        "                retmax = total_count\n",
        "\n",
        "            batch_size = 500\n",
        "            for start in range(0, retmax, batch_size):\n",
        "                params = {\n",
        "                    'db': 'clinvar',\n",
        "                    'term': search_query,\n",
        "                    'retmode': 'json',\n",
        "                    'retstart': start,\n",
        "                    'retmax': min(batch_size, retmax - start),\n",
        "                    'email': self.email\n",
        "                }\n",
        "\n",
        "                if self.api_key:\n",
        "                    params['api_key'] = self.api_key\n",
        "\n",
        "                response = requests.get(search_url, params=params, timeout=30)\n",
        "                response.raise_for_status()\n",
        "                data = response.json()\n",
        "\n",
        "                ids = data.get('esearchresult', {}).get('idlist', [])\n",
        "                variation_ids.extend(ids)\n",
        "\n",
        "                print(f\"[SEARCH] Retrieved {len(variation_ids)}/{retmax} IDs\")\n",
        "\n",
        "                if limit and len(variation_ids) >= limit:\n",
        "                    break\n",
        "\n",
        "                time.sleep(0.1)\n",
        "\n",
        "            if not limit and len(variation_ids) == total_count:\n",
        "                with open(variants_cache, 'w') as f:\n",
        "                    json.dump(variation_ids, f)\n",
        "                print(f\"[CACHE] Saved complete variant list\")\n",
        "\n",
        "            return variation_ids[:limit] if limit else variation_ids\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Search failed: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _rate_limit_wait(self):\n",
        "        \"\"\"\n",
        "        Thread-safe rate limiting\n",
        "        \"\"\"\n",
        "        with self.rate_limit_lock:\n",
        "            current_time = time.time()\n",
        "            # Remove requests older than 1 second\n",
        "            self.last_request_times = [t for t in self.last_request_times if current_time - t < 1.0]\n",
        "\n",
        "            # If made too many requests, wait\n",
        "            if len(self.last_request_times) >= self.requests_per_second:\n",
        "                sleep_time = 1.0 - (current_time - self.last_request_times[0])\n",
        "                if sleep_time > 0:\n",
        "                    time.sleep(sleep_time)\n",
        "\n",
        "            # Record this request\n",
        "            self.last_request_times.append(time.time())\n",
        "\n",
        "    def fetch_single_variant_parallel(self, variation_id: str, retry_count: int = 3) -> Optional[Dict]:\n",
        "        \"\"\"\n",
        "        Fetch and parse a single variant, with parallel execution\n",
        "        \"\"\"\n",
        "        # Check parsed cache first\n",
        "        parsed_cache_file = self.parsed_cache_dir / f\"{variation_id}.pkl\"\n",
        "        if parsed_cache_file.exists():\n",
        "            try:\n",
        "                with open(parsed_cache_file, 'rb') as f:\n",
        "                    return pickle.load(f)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Check XML cache\n",
        "        xml_cache_file = self.xml_cache_dir / f\"{variation_id}.xml.gz\"\n",
        "        if xml_cache_file.exists():\n",
        "            try:\n",
        "                with gzip.open(xml_cache_file, 'rt', encoding='utf-8') as f:\n",
        "                    xml_content = f.read()\n",
        "                    # Parse and cache result\n",
        "                    result = self.parse_vcv_xml(xml_content, variation_id)\n",
        "                    if result and result.get('submissions'):\n",
        "                        with open(parsed_cache_file, 'wb') as f:\n",
        "                            pickle.dump(result, f)\n",
        "                    return result\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Fetch from API\n",
        "        for attempt in range(retry_count):\n",
        "            try:\n",
        "                self._rate_limit_wait()\n",
        "\n",
        "                fetch_url = f\"{self.eutils_base}/efetch.fcgi\"\n",
        "                params = {\n",
        "                    'db': 'clinvar',\n",
        "                    'rettype': 'vcv',\n",
        "                    'id': variation_id,\n",
        "                    'is_variationid': '',\n",
        "                    'email': self.email\n",
        "                }\n",
        "\n",
        "                if self.api_key:\n",
        "                    params['api_key'] = self.api_key\n",
        "\n",
        "                response = requests.get(fetch_url, params=params, timeout=30)\n",
        "\n",
        "                if response.status_code == 200 and 'VariationArchive' in response.text:\n",
        "                    xml_content = response.text\n",
        "\n",
        "                    # Cache XML\n",
        "                    with gzip.open(xml_cache_file, 'wt', encoding='utf-8') as f:\n",
        "                        f.write(xml_content)\n",
        "\n",
        "                    result = self.parse_vcv_xml(xml_content, variation_id)\n",
        "                    if result and result.get('submissions'):\n",
        "                        with open(parsed_cache_file, 'wb') as f:\n",
        "                            pickle.dump(result, f)\n",
        "\n",
        "                    return result\n",
        "                elif response.status_code == 429:  # Rate limited\n",
        "                    time.sleep(2 ** attempt)\n",
        "                    continue\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                if attempt < retry_count - 1:\n",
        "                    time.sleep(1)\n",
        "                    continue\n",
        "            except Exception as e:\n",
        "                if attempt == retry_count - 1:\n",
        "                    print(f\"[ERROR] Failed to fetch {variation_id} after {retry_count} attempts: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def process_variants_parallel(self, variation_ids: List[str], max_workers: int = 10) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Both fetching and parsing in parallel\n",
        "        \"\"\"\n",
        "        print(f\"\\n[PARALLEL] Processing {len(variation_ids)} variants with {max_workers} workers\")\n",
        "\n",
        "        results = []\n",
        "        processed_count = 0\n",
        "        failed_count = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Use ThreadPoolExecutor for parallel processing\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            future_to_id = {\n",
        "                executor.submit(self.fetch_single_variant_parallel, var_id): var_id\n",
        "                for var_id in variation_ids\n",
        "            }\n",
        "\n",
        "            for future in as_completed(future_to_id):\n",
        "                var_id = future_to_id[future]\n",
        "                try:\n",
        "                    result = future.result(timeout=60)\n",
        "                    if result and result.get('submissions'):\n",
        "                        results.append(result)\n",
        "                        processed_count += 1\n",
        "                    else:\n",
        "                        failed_count += 1\n",
        "\n",
        "                    # Progress update\n",
        "                    total_done = processed_count + failed_count\n",
        "                    if total_done % 100 == 0:\n",
        "                        elapsed = time.time() - start_time\n",
        "                        rate = total_done / elapsed\n",
        "                        remaining = (len(variation_ids) - total_done) / rate\n",
        "                        print(f\"[PARALLEL] Progress: {total_done}/{len(variation_ids)} \"\n",
        "                              f\"({100*total_done/len(variation_ids):.1f}%) - \"\n",
        "                              f\"Rate: {rate:.1f} variants/sec - \"\n",
        "                              f\"ETA: {remaining/60:.1f} minutes\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"[ERROR] Failed to process {var_id}: {e}\")\n",
        "                    failed_count += 1\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"[PARALLEL] Completed in {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
        "        print(f\"[PARALLEL] Successfully processed: {processed_count}\")\n",
        "        print(f\"[PARALLEL] Failed: {failed_count}\")\n",
        "        print(f\"[PARALLEL] Average rate: {len(variation_ids)/elapsed:.1f} variants/second\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def parse_vcv_xml(self, xml_content: str, variation_id: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Parse VCV XML and extract relevant information\n",
        "        \"\"\"\n",
        "        try:\n",
        "            root = ET.fromstring(xml_content)\n",
        "\n",
        "            result = {\n",
        "                'variation_id': variation_id,\n",
        "                'name': '',\n",
        "                'rsid': '',\n",
        "                'submissions': [],\n",
        "                'classification_timeline': [],\n",
        "                'all_citations': {}\n",
        "            }\n",
        "\n",
        "            # Get variant basic info\n",
        "            var_archive = root.find('.//VariationArchive')\n",
        "            if var_archive is None:\n",
        "                var_archive = root\n",
        "\n",
        "            result['vcv_accession'] = var_archive.get('Accession', '')\n",
        "\n",
        "            # Get variant name - try multiple locations\n",
        "            # First try the VariationName attribute\n",
        "            var_name = var_archive.get('VariationName', '').strip()\n",
        "            if var_name:\n",
        "                result['name'] = var_name\n",
        "            else:\n",
        "                # Try HGVS - get the actual text, not whitespace\n",
        "                for hgvs_elem in root.findall('.//SimpleAllele/HGVSlist/HGVS'):\n",
        "                    if hgvs_elem.text and hgvs_elem.text.strip():\n",
        "                        result['name'] = hgvs_elem.text.strip()\n",
        "                        break\n",
        "\n",
        "                if not result['name']:\n",
        "                    # Try CanonicalSPDI\n",
        "                    spdi_elem = root.find('.//CanonicalSPDI')\n",
        "                    if spdi_elem is not None and spdi_elem.text and spdi_elem.text.strip():\n",
        "                        result['name'] = spdi_elem.text.strip()\n",
        "\n",
        "                    if not result['name']:\n",
        "                        # Try ProteinChange\n",
        "                        protein_elem = root.find('.//ProteinChange')\n",
        "                        if protein_elem is not None and protein_elem.text and protein_elem.text.strip():\n",
        "                            result['name'] = protein_elem.text.strip()\n",
        "\n",
        "            # Get rsID - check multiple locations\n",
        "            xref_elem = root.find('.//SimpleAllele/XRefList/XRef[@DB=\"dbSNP\"]')\n",
        "            if xref_elem is not None:\n",
        "                rs_id = xref_elem.get('ID', '')\n",
        "                if rs_id:\n",
        "                    if rs_id.startswith('rs'):\n",
        "                        result['rsid'] = rs_id\n",
        "                    else:\n",
        "                        result['rsid'] = f\"rs{rs_id}\"\n",
        "\n",
        "            # Parse ALL ClinicalAssertion elements\n",
        "            assertions = root.findall('.//ClinicalAssertion')\n",
        "\n",
        "            for assertion in assertions:\n",
        "                submission = self.parse_clinical_assertion(assertion)\n",
        "                if submission:\n",
        "                    result['submissions'].append(submission)\n",
        "                    # Collect citations\n",
        "                    for cit in submission.get('citations', []):\n",
        "                        if cit.get('pmid'):\n",
        "                            result['all_citations'][cit['pmid']] = cit\n",
        "\n",
        "            # Sort submissions by date\n",
        "            result['submissions'].sort(key=lambda x: x.get('date_last_evaluated', ''))\n",
        "\n",
        "            # Build classification timeline\n",
        "            # Check if include conflicts (from setting stored during process call)\n",
        "            include_conflicts = getattr(self, 'include_conflicts_setting', True)\n",
        "            result['classification_timeline'] = self.build_classification_timeline(\n",
        "                result['submissions'],\n",
        "                include_conflicts=include_conflicts\n",
        "            )\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            return {}\n",
        "\n",
        "    def parse_clinical_assertion(self, assertion_elem) -> Dict:\n",
        "        \"\"\"\n",
        "        Parse ClinicalAssertion\n",
        "        \"\"\"\n",
        "        submission = {}\n",
        "\n",
        "        try:\n",
        "            # SCV Accession\n",
        "            scv_elem = assertion_elem.find('.//ClinVarAccession[@Type=\"SCV\"]')\n",
        "            if scv_elem is not None:\n",
        "                submission['scv_accession'] = f\"{scv_elem.get('Accession')}.{scv_elem.get('Version')}\"\n",
        "                submission['submitter'] = scv_elem.get('SubmitterName', '') or scv_elem.get('OrgAbbreviation', '')\n",
        "                submission['date_updated'] = scv_elem.get('DateUpdated', '')\n",
        "\n",
        "            # Classification - check multiple locations\n",
        "            classification_found = False\n",
        "\n",
        "            # Classification/GermlineClassification\n",
        "            class_elem = assertion_elem.find('.//Classification')\n",
        "            if class_elem is not None:\n",
        "                submission['date_last_evaluated'] = class_elem.get('DateLastEvaluated', '')\n",
        "                germ_class = class_elem.find('./GermlineClassification')\n",
        "                if germ_class is not None and germ_class.text:\n",
        "                    submission['classification'] = germ_class.text.strip()\n",
        "                    classification_found = True\n",
        "                else:\n",
        "                    som_class = class_elem.find('./SomaticClinicalImpact')\n",
        "                    if som_class is not None and som_class.text:\n",
        "                        submission['classification'] = som_class.text.strip()\n",
        "                        classification_found = True\n",
        "\n",
        "                review_elem = class_elem.find('./ReviewStatus')\n",
        "                if review_elem is not None and review_elem.text:\n",
        "                    submission['review_status'] = review_elem.text.strip()\n",
        "\n",
        "            # ClinicalSignificance/Description\n",
        "            if not classification_found:\n",
        "                clin_sig_elem = assertion_elem.find('.//ClinicalSignificance')\n",
        "                if clin_sig_elem is not None:\n",
        "                    desc_elem = clin_sig_elem.find('./Description')\n",
        "                    if desc_elem is not None and desc_elem.text:\n",
        "                        submission['classification'] = desc_elem.text.strip()\n",
        "                        classification_found = True\n",
        "                    if not submission.get('date_last_evaluated'):\n",
        "                        submission['date_last_evaluated'] = clin_sig_elem.get('DateLastEvaluated', '')\n",
        "                    if not submission.get('review_status'):\n",
        "                        review_elem = clin_sig_elem.find('./ReviewStatus')\n",
        "                        if review_elem is not None and review_elem.text:\n",
        "                            submission['review_status'] = review_elem.text.strip()\n",
        "\n",
        "            # Interpretation\n",
        "            if not classification_found:\n",
        "                interp_elem = assertion_elem.find('.//Interpretation')\n",
        "                if interp_elem is not None:\n",
        "                    for child in ['Description', 'Classification']:\n",
        "                        elem = interp_elem.find(f'./{child}')\n",
        "                        if elem is not None and elem.text:\n",
        "                            submission['classification'] = elem.text.strip()\n",
        "                            classification_found = True\n",
        "                            break\n",
        "                    if not submission.get('date_last_evaluated'):\n",
        "                        submission['date_last_evaluated'] = interp_elem.get('DateLastEvaluated', '')\n",
        "\n",
        "            # Additional submitter info\n",
        "            sub_id_elem = assertion_elem.find('.//ClinVarSubmissionID')\n",
        "            if sub_id_elem is not None:\n",
        "                if not submission.get('submitter'):\n",
        "                    submission['submitter'] = sub_id_elem.get('submitter', '').replace(';', ',')\n",
        "\n",
        "            # Condition\n",
        "            trait_elem = assertion_elem.find('.//TraitSet[@Type=\"Disease\"]/Trait/Name/ElementValue[@Type=\"Preferred\"]')\n",
        "            if trait_elem is None:\n",
        "                trait_elem = assertion_elem.find('.//TraitSet/Trait/Name/ElementValue')\n",
        "            if trait_elem is not None and trait_elem.text:\n",
        "                submission['condition'] = trait_elem.text\n",
        "\n",
        "            # Citations\n",
        "            citations = []\n",
        "            for citation in assertion_elem.findall('.//Citation'):\n",
        "                pmid_elem = citation.find('./ID[@Source=\"PubMed\"]')\n",
        "                if pmid_elem is not None and pmid_elem.text:\n",
        "                    cit = {'pmid': pmid_elem.text}\n",
        "                    title_elem = citation.find('./Title')\n",
        "                    if title_elem is not None:\n",
        "                        cit['title'] = title_elem.text\n",
        "                    citations.append(cit)\n",
        "\n",
        "            submission['citations'] = citations\n",
        "            submission['citation_count'] = len(citations)\n",
        "\n",
        "            # Date fallback\n",
        "            if not submission.get('date_last_evaluated'):\n",
        "                submission['date_last_evaluated'] = submission.get('date_updated', '')\n",
        "\n",
        "            if submission.get('scv_accession'):\n",
        "                return submission\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return None\n",
        "\n",
        "    def build_classification_timeline(self, submissions: List[Dict], include_conflicts: bool = True) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Build timeline of classification changes\n",
        "        Args:\n",
        "            submissions: List of submission dictionaries\n",
        "            include_conflicts: If True, include conflicting interpretations between labs\n",
        "        \"\"\"\n",
        "        timeline = []\n",
        "\n",
        "        if len(submissions) < 2:\n",
        "            return timeline\n",
        "\n",
        "        # Track changes within same submitter over time\n",
        "        by_submitter = defaultdict(list)\n",
        "        for sub in submissions:\n",
        "            submitter = sub.get('submitter', 'Unknown')\n",
        "            if submitter and submitter != 'Unknown':\n",
        "                by_submitter[submitter].append(sub)\n",
        "\n",
        "        # Find temporal changes within each submitter\n",
        "        for submitter, subs in by_submitter.items():\n",
        "            if len(subs) < 2:\n",
        "                continue\n",
        "\n",
        "            # Sort by date\n",
        "            subs.sort(key=lambda x: x.get('date_last_evaluated', ''))\n",
        "\n",
        "            for i in range(1, len(subs)):\n",
        "                prev_sub = subs[i-1]\n",
        "                curr_sub = subs[i]\n",
        "\n",
        "                # Both must have dates for temporal comparison\n",
        "                if not prev_sub.get('date_last_evaluated') or not curr_sub.get('date_last_evaluated'):\n",
        "                    continue\n",
        "\n",
        "                prev_class = prev_sub.get('classification', '').strip()\n",
        "                curr_class = curr_sub.get('classification', '').strip()\n",
        "\n",
        "                # Skip if no classification data\n",
        "                if not prev_class or not curr_class:\n",
        "                    continue\n",
        "\n",
        "                # Normalize for comparison\n",
        "                if prev_class.lower() == curr_class.lower():\n",
        "                    continue\n",
        "\n",
        "                # A real temporal change\n",
        "                has_citations = (len(prev_sub.get('citations', [])) > 0 or\n",
        "                               len(curr_sub.get('citations', [])) > 0)\n",
        "\n",
        "                timeline.append({\n",
        "                    'submitter': submitter,\n",
        "                    'date_old': prev_sub.get('date_last_evaluated', ''),\n",
        "                    'classification_old': prev_sub.get('classification', ''),\n",
        "                    'date_new': curr_sub.get('date_last_evaluated', ''),\n",
        "                    'classification_new': curr_sub.get('classification', ''),\n",
        "                    'scv_old': prev_sub.get('scv_accession', ''),\n",
        "                    'scv_new': curr_sub.get('scv_accession', ''),\n",
        "                    'citations_old': prev_sub.get('citations', []),\n",
        "                    'citations_new': curr_sub.get('citations', []),\n",
        "                    'has_citations': has_citations,\n",
        "                    'change_type': self.classify_change(\n",
        "                        prev_sub.get('classification', ''),\n",
        "                        curr_sub.get('classification', '')\n",
        "                    ),\n",
        "                    'is_conflict': False\n",
        "                })\n",
        "\n",
        "        # Add conflicting interpretations (current disagreements between labs)\n",
        "        if include_conflicts and len(by_submitter) > 1:\n",
        "            # Get the most recent submission from each submitter\n",
        "            latest_by_submitter = {}\n",
        "            for submitter, subs in by_submitter.items():\n",
        "                if subs and submitter and submitter != 'Unknown':\n",
        "                    # Get the most recent submission with a classification\n",
        "                    for sub in reversed(sorted(subs, key=lambda x: x.get('date_last_evaluated', ''))):\n",
        "                        if sub.get('classification'):\n",
        "                            latest_by_submitter[submitter] = sub\n",
        "                            break\n",
        "\n",
        "            # Check for conflicts between submitters\n",
        "            if len(latest_by_submitter) > 1:\n",
        "                classifications = {}\n",
        "                for submitter, sub in latest_by_submitter.items():\n",
        "                    class_norm = self.normalize_classification_category(sub.get('classification', ''))\n",
        "                    if class_norm and class_norm != 'not_provided':\n",
        "                        if class_norm not in classifications:\n",
        "                            classifications[class_norm] = []\n",
        "                        classifications[class_norm].append((submitter, sub))\n",
        "\n",
        "                # Only add ONE conflict entry per variant if there are disagreements\n",
        "                if len(classifications) > 1:\n",
        "                    # Find the most significant conflict\n",
        "                    has_pathogenic = any('pathogenic' in c for c in classifications.keys())\n",
        "                    has_benign = any('benign' in c for c in classifications.keys())\n",
        "                    has_uncertain = 'uncertain_significance' in classifications.keys()\n",
        "\n",
        "                    # Only add if it's a significant conflict\n",
        "                    if (has_pathogenic and has_benign) or (has_pathogenic and has_uncertain) or (has_benign and has_uncertain):\n",
        "                        # Get representatives from each category\n",
        "                        conflict_summary = []\n",
        "                        for class_type in sorted(classifications.keys()):\n",
        "                            if classifications[class_type]:\n",
        "                                submitter, sub = classifications[class_type][0]\n",
        "                                conflict_summary.append(f\"{submitter}: {sub.get('classification')}\")\n",
        "\n",
        "                        # Add single conflict entry\n",
        "                        timeline.append({\n",
        "                            'submitter': 'CONFLICT_BETWEEN_LABS',\n",
        "                            'date_old': '',\n",
        "                            'classification_old': 'Conflicting interpretations',\n",
        "                            'date_new': datetime.now().strftime('%Y-%m-%d'),\n",
        "                            'classification_new': ' | '.join(conflict_summary[:3]),  # Limit to 3 examples\n",
        "                            'scv_old': '',\n",
        "                            'scv_new': '',\n",
        "                            'citations_old': [],\n",
        "                            'citations_new': [],\n",
        "                            'has_citations': False,\n",
        "                            'change_type': 'conflicting_interpretations',\n",
        "                            'is_conflict': True\n",
        "                        })\n",
        "\n",
        "        return timeline\n",
        "\n",
        "    def normalize_classification_category(self, classification: str) -> str:\n",
        "        \"\"\"\n",
        "        Normalize classification to major categories for conflict detection\n",
        "        \"\"\"\n",
        "        class_lower = classification.lower().strip()\n",
        "\n",
        "        if 'pathogenic' in class_lower and 'likely' in class_lower:\n",
        "            return 'likely_pathogenic'\n",
        "        elif 'pathogenic' in class_lower:\n",
        "            return 'pathogenic'\n",
        "        elif 'benign' in class_lower and 'likely' in class_lower:\n",
        "            return 'likely_benign'\n",
        "        elif 'benign' in class_lower:\n",
        "            return 'benign'\n",
        "        elif 'uncertain' in class_lower or 'vus' in class_lower:\n",
        "            return 'uncertain_significance'\n",
        "        elif 'not provided' in class_lower:\n",
        "            return 'not_provided'\n",
        "        else:\n",
        "            return class_lower\n",
        "\n",
        "    def classify_change(self, old_class: str, new_class: str) -> str:\n",
        "        \"\"\"\n",
        "        Classify the type of classification change\n",
        "        \"\"\"\n",
        "        old_lower = old_class.lower()\n",
        "        new_lower = new_class.lower()\n",
        "\n",
        "        pathogenic = ['pathogenic', 'likely pathogenic']\n",
        "        uncertain = ['uncertain significance', 'variant of uncertain significance', 'vus']\n",
        "        benign = ['benign', 'likely benign']\n",
        "\n",
        "        old_category = None\n",
        "        new_category = None\n",
        "\n",
        "        for category, terms in [('pathogenic', pathogenic),\n",
        "                                ('uncertain', uncertain),\n",
        "                                ('benign', benign)]:\n",
        "            if any(term in old_lower for term in terms):\n",
        "                old_category = category\n",
        "            if any(term in new_lower for term in terms):\n",
        "                new_category = category\n",
        "\n",
        "        if old_category == 'uncertain' and new_category == 'pathogenic':\n",
        "            return 'upgraded_to_pathogenic'\n",
        "        elif old_category == 'uncertain' and new_category == 'benign':\n",
        "            return 'downgraded_to_benign'\n",
        "        elif old_category == 'pathogenic' and new_category == 'uncertain':\n",
        "            return 'downgraded_to_uncertain'\n",
        "        elif old_category == 'benign' and new_category == 'uncertain':\n",
        "            return 'upgraded_to_uncertain'\n",
        "        elif old_category == 'pathogenic' and new_category == 'benign':\n",
        "            return 'major_downgrade'\n",
        "        elif old_category == 'benign' and new_category == 'pathogenic':\n",
        "            return 'major_upgrade'\n",
        "        else:\n",
        "            return 'other_change'\n",
        "\n",
        "    def get_statistics(self, limit: int = 1000) -> Dict:\n",
        "        \"\"\"\n",
        "        Get quick statistics about BRCA1 variants\n",
        "        \"\"\"\n",
        "        # Check cached stats\n",
        "        if self.stats_cache.exists():\n",
        "            with open(self.stats_cache, 'r') as f:\n",
        "                stats = json.load(f)\n",
        "                print(\"[STATS] Using cached statistics\")\n",
        "                return stats\n",
        "\n",
        "        print(f\"\\n[STATS] Analyzing first {limit} variants for statistics...\")\n",
        "\n",
        "        variant_ids = self.search_brca1_variants(limit=limit)\n",
        "\n",
        "        results = self.process_variants_parallel(variant_ids, max_workers=10)\n",
        "\n",
        "        stats = {\n",
        "            'total_variants_analyzed': len(results),\n",
        "            'variants_with_multiple_submissions': 0,\n",
        "            'variants_with_changes': 0,\n",
        "            'variants_with_changes_and_citations': 0,\n",
        "            'total_submissions': 0,\n",
        "            'unique_submitters': set(),\n",
        "            'classification_distribution': defaultdict(int),\n",
        "            'change_type_distribution': defaultdict(int)\n",
        "        }\n",
        "\n",
        "        for result in results:\n",
        "            num_subs = len(result['submissions'])\n",
        "            stats['total_submissions'] += num_subs\n",
        "\n",
        "            if num_subs > 1:\n",
        "                stats['variants_with_multiple_submissions'] += 1\n",
        "\n",
        "            for sub in result['submissions']:\n",
        "                if sub.get('submitter'):\n",
        "                    stats['unique_submitters'].add(sub['submitter'])\n",
        "                if sub.get('classification'):\n",
        "                    stats['classification_distribution'][sub['classification']] += 1\n",
        "\n",
        "            if result['classification_timeline']:\n",
        "                stats['variants_with_changes'] += 1\n",
        "\n",
        "                # Check if any change has citations\n",
        "                has_citations = any(\n",
        "                    len(change.get('citations_old', [])) > 0 or\n",
        "                    len(change.get('citations_new', [])) > 0\n",
        "                    for change in result['classification_timeline']\n",
        "                )\n",
        "                if has_citations:\n",
        "                    stats['variants_with_changes_and_citations'] += 1\n",
        "\n",
        "                for change in result['classification_timeline']:\n",
        "                    stats['change_type_distribution'][change['change_type']] += 1\n",
        "\n",
        "        stats['unique_submitters'] = len(stats['unique_submitters'])\n",
        "        stats['classification_distribution'] = dict(stats['classification_distribution'])\n",
        "        stats['change_type_distribution'] = dict(stats['change_type_distribution'])\n",
        "\n",
        "\n",
        "        with open(self.stats_cache, 'w') as f:\n",
        "            json.dump(stats, f, indent=2)\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def process_all_brca1_variants(self, limit: int = None, max_workers: int = 10,\n",
        "                                   require_citations: bool = True, include_conflicts: bool = True):\n",
        "        \"\"\"\n",
        "        Main processing method with TRUE parallel processing\n",
        "        Args:\n",
        "            limit: Number of variants to process\n",
        "            max_workers: Number of parallel workers\n",
        "            require_citations: If True, only include changes with citations; if False, include all changes\n",
        "            include_conflicts: If True, include conflicting interpretations between labs\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"TRULY PARALLEL PROCESSING - ALL BRCA1 VARIANTS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        self.include_conflicts_setting = include_conflicts\n",
        "\n",
        "        variation_ids = self.search_brca1_variants(limit=limit)\n",
        "\n",
        "        print(f\"\\n[PROCESS] Processing {len(variation_ids)} variants...\")\n",
        "        print(f\"[OPTIMIZE] Using {max_workers} parallel workers for API fetching AND parsing\")\n",
        "        print(f\"[FILTER] Require citations: {require_citations}\")\n",
        "        print(f\"[FILTER] Include conflicts: {include_conflicts}\")\n",
        "\n",
        "        all_results = self.process_variants_parallel(variation_ids, max_workers=max_workers)\n",
        "\n",
        "        # Filter for variants with changes\n",
        "        variants_with_changes = []\n",
        "        variants_with_changes_no_citations = []\n",
        "        all_pmids = set()\n",
        "\n",
        "        for result in all_results:\n",
        "            if result.get('classification_timeline'):\n",
        "                # Filter out conflicts if not wanted\n",
        "                if not include_conflicts:\n",
        "                    timeline_filtered = [\n",
        "                        change for change in result['classification_timeline']\n",
        "                        if not change.get('is_conflict', False)\n",
        "                    ]\n",
        "                else:\n",
        "                    timeline_filtered = result['classification_timeline']\n",
        "\n",
        "                # Skip if no changes after filtering\n",
        "                if not timeline_filtered:\n",
        "                    continue\n",
        "\n",
        "                # Separate changes with and without citations\n",
        "                changes_with_citations = [\n",
        "                    change for change in timeline_filtered\n",
        "                    if change.get('has_citations', False)\n",
        "                ]\n",
        "\n",
        "                changes_without_citations = [\n",
        "                    change for change in timeline_filtered\n",
        "                    if not change.get('has_citations', False)\n",
        "                ]\n",
        "\n",
        "                # Collect PMIDs\n",
        "                for sub in result['submissions']:\n",
        "                    for cit in sub.get('citations', []):\n",
        "                        if cit.get('pmid'):\n",
        "                            all_pmids.add(cit['pmid'])\n",
        "\n",
        "                variant_record = {\n",
        "                    'variation_id': result['variation_id'],\n",
        "                    'vcv_accession': result.get('vcv_accession', ''),\n",
        "                    'rsid': result.get('rsid', ''),\n",
        "                    'gene': 'BRCA1',\n",
        "                    'variant_name': result.get('name', '').strip(),\n",
        "                    'total_submissions': len(result['submissions']),\n",
        "                    'all_submissions': [\n",
        "                        {\n",
        "                            'scv_accession': sub.get('scv_accession', ''),\n",
        "                            'submitter': sub.get('submitter', ''),\n",
        "                            'date': sub.get('date_last_evaluated', ''),\n",
        "                            'classification': sub.get('classification', ''),\n",
        "                            'review_status': sub.get('review_status', ''),\n",
        "                            'citation_count': sub.get('citation_count', 0)\n",
        "                        } for sub in result['submissions']\n",
        "                    ]\n",
        "                }\n",
        "\n",
        "                if require_citations and changes_with_citations:\n",
        "                    variant_record['classification_changes'] = changes_with_citations\n",
        "                    variants_with_changes.append(variant_record)\n",
        "                elif not require_citations and timeline_filtered:\n",
        "                    variant_record['classification_changes'] = timeline_filtered\n",
        "                    variants_with_changes.append(variant_record)\n",
        "\n",
        "                if changes_without_citations:\n",
        "                    variants_with_changes_no_citations.append(variant_record)\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        output_data = {\n",
        "            'metadata': {\n",
        "                'gene': 'BRCA1',\n",
        "                'extraction_date': datetime.now().isoformat(),\n",
        "                'processing_time_seconds': processing_time,\n",
        "                'processing_time_minutes': processing_time / 60,\n",
        "                'total_variants_processed': len(all_results),\n",
        "                'variants_with_changes': len(variants_with_changes),\n",
        "                'variants_with_changes_no_citations': len(variants_with_changes_no_citations),\n",
        "                'unique_citations': len(all_pmids),\n",
        "                'processing_rate': len(all_results) / processing_time if processing_time > 0 else 0,\n",
        "                'parallel_workers': max_workers,\n",
        "                'citation_filter_applied': require_citations,\n",
        "                'conflicts_included': include_conflicts\n",
        "            },\n",
        "            'variants': variants_with_changes\n",
        "        }\n",
        "\n",
        "        output_file = f\"BRCA1_changes_parallel_{timestamp}.json\"\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(output_data, f, indent=2, ensure_ascii=False, default=str)\n",
        "\n",
        "        print(f\"\\n[SAVE] Results saved to: {output_file}\")\n",
        "\n",
        "        self.print_summary(output_data)\n",
        "\n",
        "        return output_data\n",
        "\n",
        "    def print_summary(self, data: Dict):\n",
        "        \"\"\"\n",
        "        Print analysis summary\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ANALYSIS COMPLETE\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        metadata = data['metadata']\n",
        "        variants = data['variants']\n",
        "\n",
        "        print(f\"\\n PERFORMANCE:\")\n",
        "        print(f\"  Processing time: {metadata['processing_time_seconds']:.1f} seconds\")\n",
        "        print(f\"  Processing time: {metadata['processing_time_minutes']:.1f} minutes\")\n",
        "        print(f\"  Variants processed: {metadata['total_variants_processed']}\")\n",
        "        print(f\"  Processing rate: {metadata['processing_rate']:.1f} variants/second\")\n",
        "        print(f\"  Parallel workers used: {metadata['parallel_workers']}\")\n",
        "\n",
        "        print(f\"\\n RESULTS:\")\n",
        "        print(f\"  Variants with changes: {len(variants)}\")\n",
        "        print(f\"  Percentage with changes: {100*len(variants)/metadata['total_variants_processed']:.1f}%\")\n",
        "\n",
        "        if variants:\n",
        "            # Count change types\n",
        "            change_types = defaultdict(int)\n",
        "            for var in variants:\n",
        "                for change in var['classification_changes']:\n",
        "                    change_types[change['change_type']] += 1\n",
        "\n",
        "            print(f\"\\n  Change type distribution:\")\n",
        "            for change_type, count in sorted(change_types.items(), key=lambda x: x[1], reverse=True):\n",
        "                print(f\"    {change_type}: {count}\")\n",
        "\n",
        "            # Sample changes\n",
        "            print(f\"\\n  Sample classification changes (first 3):\")\n",
        "            for var in variants[:3]:\n",
        "                print(f\"\\n    Variant ID: {var['variation_id']}\")\n",
        "                print(f\"    Name: {var['variant_name'] or 'N/A'}\")\n",
        "                print(f\"    RS: {var['rsid'] or 'N/A'}\")\n",
        "                for change in var['classification_changes'][:1]:\n",
        "                    print(f\"    Change by {change['submitter']}:\")\n",
        "                    print(f\"      {change['classification_old']} → {change['classification_new']}\")\n",
        "                    print(f\"      ({change['date_old']} to {change['date_new']})\")\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "\n",
        "    EMAIL = \"yl8889@nyu.edu\"\n",
        "    API_KEY = \"92c067f1875f6abf0dfbf5d0d57758beff09\"\n",
        "\n",
        "    # Options: 100 (test), None (all ~15,000)\n",
        "    LIMIT_VARIANTS = None\n",
        "\n",
        "    # Number of parallel workers for API fetching\n",
        "    MAX_WORKERS = 15\n",
        "\n",
        "    # Citation filter\n",
        "    # True = Only changes WITH citations (~10-50 variants expected)\n",
        "    # False = ALL classification changes (~1000-2000 variants expected)\n",
        "    REQUIRE_CITATIONS = False\n",
        "\n",
        "    # Include conflicting interpretations between labs\n",
        "    # True = Include variants where labs currently disagree\n",
        "    # False = Only temporal changes (same lab changing over time)\n",
        "    INCLUDE_CONFLICTS = False  # Set to False to exclude conflicts\n",
        "\n",
        "    # Show statistics before processing\n",
        "    SHOW_STATISTICS = True\n",
        "\n",
        "    # Statistics sample size (if SHOW_STATISTICS is True)\n",
        "    STATS_SAMPLE_SIZE = 1000\n",
        "\n",
        "\n",
        "    print(\"\\n CONFIGURATION:\")\n",
        "    print(f\"  Variants to process: {LIMIT_VARIANTS if LIMIT_VARIANTS else 'ALL (~15,000)'}\")\n",
        "    print(f\"  Parallel workers: {MAX_WORKERS}\")\n",
        "    print(f\"  Require citations: {REQUIRE_CITATIONS}\")\n",
        "    print(f\"  Include conflicts between labs: {INCLUDE_CONFLICTS}\")\n",
        "    print(f\"  Show statistics: {SHOW_STATISTICS}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    api = ClinVarParallelAPI(email=EMAIL, api_key=API_KEY)\n",
        "\n",
        "    if SHOW_STATISTICS:\n",
        "        print(f\"\\n Generating statistics from {STATS_SAMPLE_SIZE} variants...\")\n",
        "        stats = api.get_statistics(limit=STATS_SAMPLE_SIZE)\n",
        "        print(f\"\\n BRCA1 VARIANT STATISTICS (based on {STATS_SAMPLE_SIZE} variants):\")\n",
        "        print(f\"  Variants analyzed: {stats['total_variants_analyzed']}\")\n",
        "        print(f\"  Variants with multiple submissions: {stats['variants_with_multiple_submissions']}\")\n",
        "        print(f\"  Variants with changes: {stats['variants_with_changes']}\")\n",
        "        print(f\"  Variants with changes + citations: {stats['variants_with_changes_and_citations']}\")\n",
        "\n",
        "        if stats['total_variants_analyzed'] > 0:\n",
        "            print(f\"\\n  Expected yields:\")\n",
        "            print(f\"    With citations filter: ~{100*stats['variants_with_changes_and_citations']/stats['total_variants_analyzed']:.2f}% of variants\")\n",
        "            print(f\"    Without citations filter: ~{100*stats['variants_with_changes']/stats['total_variants_analyzed']:.2f}% of variants\")\n",
        "\n",
        "            if LIMIT_VARIANTS:\n",
        "                est_with_citations = int(LIMIT_VARIANTS * stats['variants_with_changes_and_citations'] / stats['total_variants_analyzed'])\n",
        "                est_without_citations = int(LIMIT_VARIANTS * stats['variants_with_changes'] / stats['total_variants_analyzed'])\n",
        "                print(f\"\\n  Estimated results for {LIMIT_VARIANTS} variants:\")\n",
        "                print(f\"    With citations: ~{est_with_citations} variants\")\n",
        "                print(f\"    Without citations: ~{est_without_citations} variants\")\n",
        "\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "\n",
        "    print(f\"\\n Starting processing...\")\n",
        "    print(f\"  Processing {LIMIT_VARIANTS if LIMIT_VARIANTS else 'ALL'} variants\")\n",
        "    print(f\"  Using {MAX_WORKERS} parallel workers\")\n",
        "    print(f\"  Citation filter: {'ENABLED (strict)' if REQUIRE_CITATIONS else 'DISABLED (all changes)'}\")\n",
        "    print(f\"  Conflict detection: {'ENABLED' if INCLUDE_CONFLICTS else 'DISABLED (temporal changes only)'}\")\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        result_data = api.process_all_brca1_variants(\n",
        "            limit=LIMIT_VARIANTS,\n",
        "            max_workers=MAX_WORKERS,\n",
        "            require_citations=REQUIRE_CITATIONS,\n",
        "            include_conflicts=INCLUDE_CONFLICTS\n",
        "        )\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"PROCESSING COMPLETE\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"\\n FINAL RESULTS:\")\n",
        "        print(f\"  Total time: {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
        "        print(f\"  Variants processed: {result_data['metadata']['total_variants_processed']}\")\n",
        "        print(f\"  Variants with changes: {len(result_data['variants'])}\")\n",
        "        print(f\"  Processing rate: {result_data['metadata']['processing_rate']:.1f} variants/second\")\n",
        "\n",
        "        if len(result_data['variants']) > 0:\n",
        "            print(f\"\\n  Change percentage: {100*len(result_data['variants'])/result_data['metadata']['total_variants_processed']:.2f}%\")\n",
        "\n",
        "            # Count change types\n",
        "            change_types = defaultdict(int)\n",
        "            temporal_changes = 0\n",
        "            conflict_changes = 0\n",
        "            total_changes = 0\n",
        "\n",
        "            for var in result_data['variants']:\n",
        "                for change in var.get('classification_changes', []):\n",
        "                    change_types[change.get('change_type', 'unknown')] += 1\n",
        "                    total_changes += 1\n",
        "                    if change.get('is_conflict', False):\n",
        "                        conflict_changes += 1\n",
        "                    else:\n",
        "                        temporal_changes += 1\n",
        "\n",
        "            if change_types:\n",
        "                print(f\"\\n  Total classification events: {total_changes}\")\n",
        "                print(f\"    Temporal changes (same lab): {temporal_changes}\")\n",
        "                print(f\"    Conflicts (between labs): {conflict_changes}\")\n",
        "                print(f\"\\n  Change type distribution:\")\n",
        "                for change_type, count in sorted(change_types.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
        "                    print(f\"    {change_type}: {count} ({100*count/total_changes:.1f}%)\")\n",
        "\n",
        "        # Save filename from timestamp in metadata\n",
        "        output_file = f\"BRCA1_changes_parallel_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "        print(f\"\\n Results saved to: {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n Error during processing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i4TNTsn8RokQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}