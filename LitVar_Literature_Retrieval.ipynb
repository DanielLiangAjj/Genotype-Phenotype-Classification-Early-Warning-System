{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm1ntQeGYRmI",
        "outputId": "828ffe0e-27d3-4225-ed25-05a09492aa83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.5)\n",
            "Collecting bio\n",
            "  Downloading bio-1.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.1.2)\n",
            "Collecting biopython>=1.80 (from bio)\n",
            "  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting gprofiler-official (from bio)\n",
            "  Downloading gprofiler_official-1.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mygene (from bio)\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.11/dist-packages (from bio) (1.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from bio) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting biothings-client>=0.2.6 (from mygene->bio)\n",
            "  Downloading biothings_client-0.4.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch->bio) (4.3.8)\n",
            "Requirement already satisfied: httpx>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from biothings-client>=0.2.6->mygene->bio) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->bio) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.22.0->biothings-client>=0.2.6->mygene->bio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.22.0->biothings-client>=0.2.6->mygene->bio) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene->bio) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.22.0->biothings-client>=0.2.6->mygene->bio) (4.14.1)\n",
            "Downloading bio-1.8.0-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.1/321.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Downloading biothings_client-0.4.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython, gprofiler-official, biothings-client, mygene, bio\n",
            "Successfully installed bio-1.8.0 biopython-1.85 biothings-client-0.4.1 gprofiler-official-1.0.0 mygene-3.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install requests pandas numpy matplotlib seaborn plotly networkx bio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghgdod5eYBZE",
        "outputId": "d23340d1-8994-44cf-b14d-0b78acc8dbd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Enhanced LitVar Analyzer...\n",
            "Loading journal CiteScores from /content/journal_ranking_data.csv\n",
            "   Loaded CiteScores for 17145 journals\n",
            "   Highest CiteScores loaded:\n",
            "      ca-a cancer journal for clinicians: 642.9\n",
            "      ca cancer j clin: 642.9\n",
            "      ca-a cancer j clin: 642.9\n",
            "      nature reviews molecular cell biology: 164.4\n",
            "      new england journal of medicine: 134.4\n",
            "\n",
            "================================================================================\n",
            "STARTING LITVAR ANALYSIS WITH JSON INPUT\n",
            "================================================================================\n",
            "\n",
            " LOADING CLASSIFICATION CHANGES FROM JSON\n",
            "   File: /content/BRCA1_changes_parallel_20250815_234141.json\n",
            "   Loaded 6318 classification changes\n",
            "   Found 12460 total citations\n",
            "\n",
            " FETCHING PUBLICATIONS FOR ALL VARIANTS (OPTIMIZED)\n",
            "   Found 3436 unique variants to process\n",
            "   3436 variants with valid rsIDs\n",
            "\n",
            "   BATCH FETCHING FROM LITVAR2...\n",
            "\n",
            "   Processing variants 1-100 of 3436...\n",
            "         Found 43 cached, fetching 57 from LitVar2...\n",
            "         Processing chunk 1/3 (20 variants)...\n",
            "         Processing chunk 2/3 (20 variants)...\n",
            "         Processing chunk 3/3 (17 variants)...\n",
            "         Fetched data for 98 variants total\n",
            "   Progress: 100/3436 variants (2.9%)\n",
            "\n",
            "   Processing variants 101-200 of 3436...\n",
            "         Found 81 cached, fetching 16 from LitVar2...\n",
            "         Processing chunk 1/1 (16 variants)...\n",
            "         Fetched data for 97 variants total\n",
            "   Progress: 200/3436 variants (5.8%)\n",
            "\n",
            "   Processing variants 201-300 of 3436...\n",
            "         Found 57 cached, fetching 43 from LitVar2...\n",
            "         Processing chunk 1/3 (20 variants)...\n",
            "         Processing chunk 2/3 (20 variants)...\n",
            "         Processing chunk 3/3 (3 variants)...\n",
            "         Fetched data for 100 variants total\n",
            "   Progress: 300/3436 variants (8.7%)\n",
            "\n",
            "   Processing variants 301-400 of 3436...\n",
            "         Found 78 cached, fetching 20 from LitVar2...\n",
            "         Processing chunk 1/1 (20 variants)...\n",
            "         Fetched data for 98 variants total\n",
            "   Progress: 400/3436 variants (11.6%)\n",
            "\n",
            "   Processing variants 401-500 of 3436...\n",
            "         Found 42 cached, fetching 56 from LitVar2...\n",
            "         Processing chunk 1/3 (20 variants)...\n",
            "         Processing chunk 2/3 (20 variants)...\n",
            "         Processing chunk 3/3 (16 variants)...\n",
            "         Fetched data for 98 variants total\n",
            "   Progress: 500/3436 variants (14.6%)\n",
            "\n",
            "   Processing variants 501-600 of 3436...\n",
            "         Found 56 cached, fetching 44 from LitVar2...\n",
            "         Processing chunk 1/3 (20 variants)...\n",
            "         Processing chunk 2/3 (20 variants)...\n",
            "         Processing chunk 3/3 (4 variants)...\n",
            "         Fetched data for 100 variants total\n",
            "   Progress: 600/3436 variants (17.5%)\n",
            "\n",
            "   Processing variants 601-700 of 3436...\n",
            "         Found 53 cached, fetching 47 from LitVar2...\n",
            "         Processing chunk 1/3 (20 variants)...\n",
            "         Processing chunk 2/3 (20 variants)...\n",
            "         Processing chunk 3/3 (7 variants)...\n",
            "         Fetched data for 99 variants total\n",
            "   Progress: 700/3436 variants (20.4%)\n",
            "\n",
            "   Processing variants 701-800 of 3436...\n",
            "         Found 98 cached, fetching 2 from LitVar2...\n",
            "         Processing chunk 1/1 (2 variants)...\n",
            "         Fetched data for 100 variants total\n",
            "   Progress: 800/3436 variants (23.3%)\n",
            "\n",
            "   Processing variants 801-900 of 3436...\n",
            "         Found 72 cached, fetching 27 from LitVar2...\n",
            "         Processing chunk 1/2 (20 variants)...\n",
            "         Processing chunk 2/2 (7 variants)...\n",
            "         Fetched data for 99 variants total\n",
            "   Progress: 900/3436 variants (26.2%)\n",
            "\n",
            "   Processing variants 901-1000 of 3436...\n",
            "         Found 75 cached, fetching 25 from LitVar2...\n",
            "         Processing chunk 1/2 (20 variants)...\n",
            "         Processing chunk 2/2 (5 variants)...\n",
            "         Fetched data for 100 variants total\n",
            "   Progress: 1000/3436 variants (29.1%)\n",
            "\n",
            "   Processing variants 1001-1100 of 3436...\n",
            "         Found 98 cached, fetching 1 from LitVar2...\n",
            "         Processing chunk 1/1 (1 variants)...\n",
            "         Fetched data for 99 variants total\n",
            "   Progress: 1100/3436 variants (32.0%)\n",
            "\n",
            "   Processing variants 1101-1200 of 3436...\n",
            "         Found 90 cached, fetching 7 from LitVar2...\n",
            "         Processing chunk 1/1 (7 variants)...\n",
            "         Fetched data for 97 variants total\n",
            "   Progress: 1200/3436 variants (34.9%)\n",
            "\n",
            "   Processing variants 1201-1300 of 3436...\n",
            "         Found 79 cached, fetching 19 from LitVar2...\n",
            "         Processing chunk 1/1 (19 variants)...\n",
            "         Fetched data for 97 variants total\n",
            "   Progress: 1300/3436 variants (37.8%)\n",
            "\n",
            "   Processing variants 1301-1400 of 3436...\n",
            "         Found 96 cached, fetching 1 from LitVar2...\n",
            "         Processing chunk 1/1 (1 variants)...\n",
            "         Fetched data for 97 variants total\n",
            "   Progress: 1400/3436 variants (40.7%)\n",
            "\n",
            "   Processing variants 1401-1500 of 3436...\n",
            "         Found 94 cached, fetching 5 from LitVar2...\n",
            "         Processing chunk 1/1 (5 variants)...\n",
            "         Fetched data for 99 variants total\n",
            "   Progress: 1500/3436 variants (43.7%)\n",
            "\n",
            "   Processing variants 1501-1600 of 3436...\n",
            "         Found 88 cached, fetching 12 from LitVar2...\n",
            "         Processing chunk 1/1 (12 variants)...\n",
            "         Fetched data for 100 variants total\n",
            "   Progress: 1600/3436 variants (46.6%)\n",
            "\n",
            "   Processing variants 1601-1700 of 3436...\n",
            "         Found 97 cached, fetching 3 from LitVar2...\n",
            "         Processing chunk 1/1 (3 variants)...\n",
            "         Fetched data for 100 variants total\n",
            "   Progress: 1700/3436 variants (49.5%)\n",
            "\n",
            "   Processing variants 1701-1800 of 3436...\n",
            "         Found 94 cached, fetching 6 from LitVar2...\n",
            "         Processing chunk 1/1 (6 variants)...\n",
            "         Fetched data for 100 variants total\n",
            "   Progress: 1800/3436 variants (52.4%)\n",
            "\n",
            "   Processing variants 1801-1900 of 3436...\n",
            "         Found 97 cached, fetching 3 from LitVar2...\n",
            "         Processing chunk 1/1 (3 variants)...\n",
            "         Fetched data for 100 variants total\n",
            "   Progress: 1900/3436 variants (55.3%)\n",
            "\n",
            "   Processing variants 1901-2000 of 3436...\n",
            "         Found 91 cached, fetching 1 from LitVar2...\n",
            "         Processing chunk 1/1 (1 variants)...\n",
            "         Fetched data for 92 variants total\n",
            "   Progress: 2000/3436 variants (58.2%)\n",
            "\n",
            "   Processing variants 2001-2100 of 3436...\n",
            "         Found 98 cached, fetching 1 from LitVar2...\n",
            "         Processing chunk 1/1 (1 variants)...\n",
            "         Fetched data for 99 variants total\n",
            "   Progress: 2100/3436 variants (61.1%)\n",
            "\n",
            "   Processing variants 2101-2200 of 3436...\n",
            "         Found 94 cached, fetching 1 from LitVar2...\n",
            "         Processing chunk 1/1 (1 variants)...\n",
            "         Fetched data for 95 variants total\n",
            "   Progress: 2200/3436 variants (64.0%)\n",
            "\n",
            "   Processing variants 2201-2300 of 3436...\n",
            "         All 100 variants found in cache\n",
            "   Progress: 2300/3436 variants (66.9%)\n",
            "\n",
            "   Processing variants 2301-2400 of 3436...\n",
            "         All 100 variants found in cache\n",
            "   Progress: 2400/3436 variants (69.8%)\n",
            "\n",
            "   Processing variants 2401-2500 of 3436...\n",
            "         All 100 variants found in cache\n",
            "   Progress: 2500/3436 variants (72.8%)\n",
            "\n",
            "   Processing variants 2501-2600 of 3436...\n",
            "         Found 95 cached, fetching 1 from LitVar2...\n",
            "         Processing chunk 1/1 (1 variants)...\n",
            "         Fetched data for 96 variants total\n",
            "   Progress: 2600/3436 variants (75.7%)\n",
            "\n",
            "   Processing variants 2601-2700 of 3436...\n",
            "         Found 97 cached, fetching 2 from LitVar2...\n",
            "         Processing chunk 1/1 (2 variants)...\n",
            "         Fetched data for 99 variants total\n",
            "   Progress: 2700/3436 variants (78.6%)\n",
            "\n",
            "   Processing variants 2701-2800 of 3436...\n",
            "         Found 93 cached, fetching 6 from LitVar2...\n",
            "         Processing chunk 1/1 (6 variants)...\n",
            "         Fetched data for 99 variants total\n",
            "   Progress: 2800/3436 variants (81.5%)\n",
            "\n",
            "   Processing variants 2801-2900 of 3436...\n",
            "         Found 96 cached, fetching 3 from LitVar2...\n",
            "         Processing chunk 1/1 (3 variants)...\n",
            "         Fetched data for 99 variants total\n",
            "   Progress: 2900/3436 variants (84.4%)\n",
            "\n",
            "   Processing variants 2901-3000 of 3436...\n",
            "         Found 69 cached, fetching 31 from LitVar2...\n",
            "         Processing chunk 1/2 (20 variants)...\n",
            "         Processing chunk 2/2 (11 variants)...\n",
            "         Fetched data for 100 variants total\n",
            "   Progress: 3000/3436 variants (87.3%)\n",
            "\n",
            "   Processing variants 3001-3100 of 3436...\n",
            "         Found 57 cached, fetching 43 from LitVar2...\n",
            "         Processing chunk 1/3 (20 variants)...\n",
            "         Processing chunk 2/3 (20 variants)...\n",
            "         Processing chunk 3/3 (3 variants)...\n",
            "         Fetched data for 100 variants total\n",
            "   Progress: 3100/3436 variants (90.2%)\n",
            "\n",
            "   Processing variants 3101-3200 of 3436...\n",
            "         Found 60 cached, fetching 40 from LitVar2...\n",
            "         Processing chunk 1/2 (20 variants)...\n",
            "         Processing chunk 2/2 (20 variants)...\n",
            "         Fetched data for 99 variants total\n",
            "   Progress: 3200/3436 variants (93.1%)\n",
            "\n",
            "   Processing variants 3201-3300 of 3436...\n",
            "         Found 70 cached, fetching 29 from LitVar2...\n",
            "         Processing chunk 1/2 (20 variants)...\n",
            "         Processing chunk 2/2 (9 variants)...\n",
            "         Fetched data for 99 variants total\n",
            "   Progress: 3300/3436 variants (96.0%)\n",
            "\n",
            "   Processing variants 3301-3400 of 3436...\n",
            "         Found 53 cached, fetching 46 from LitVar2...\n",
            "         Processing chunk 1/3 (20 variants)...\n",
            "         Processing chunk 2/3 (20 variants)...\n",
            "         Processing chunk 3/3 (6 variants)...\n",
            "         Fetched data for 98 variants total\n",
            "   Progress: 3400/3436 variants (99.0%)\n",
            "\n",
            "   Processing variants 3401-3436 of 3436...\n",
            "         Found 23 cached, fetching 13 from LitVar2...\n",
            "         Processing chunk 1/1 (13 variants)...\n",
            "         Fetched data for 36 variants total\n",
            "\n",
            "   Completed LitVar fetching for 2860 variants\n",
            "\n",
            "   Collecting unique PMIDs...\n",
            "   Found 4026 unique PMIDs total\n",
            "\n",
            "   BATCH FETCHING PUBLICATIONS FROM PUBMED...\n",
            "   Fetching PubMed batch 1-500 of 4026...\n",
            "         Fetching batch of 500 publications from PubMed...\n",
            "   Fetching PubMed batch 501-1000 of 4026...\n",
            "         Fetching batch of 500 publications from PubMed...\n",
            "   Fetching PubMed batch 1001-1500 of 4026...\n",
            "         Fetching batch of 499 publications from PubMed...\n",
            "   Fetching PubMed batch 1501-2000 of 4026...\n",
            "         Fetching batch of 499 publications from PubMed...\n",
            "   Fetching PubMed batch 2001-2500 of 4026...\n",
            "         Fetching batch of 500 publications from PubMed...\n",
            "   Fetching PubMed batch 2501-3000 of 4026...\n",
            "         Fetching batch of 500 publications from PubMed...\n",
            "   Fetching PubMed batch 3001-3500 of 4026...\n",
            "         Fetching batch of 500 publications from PubMed...\n",
            "   Fetching PubMed batch 3501-4000 of 4026...\n",
            "         Fetching batch of 500 publications from PubMed...\n",
            "   Fetching PubMed batch 4001-4026 of 4026...\n",
            "         Fetching batch of 26 publications from PubMed...\n",
            "   Fetched details for 4026 publications\n",
            "\n",
            "   PROCESSING VARIANT DATA...\n",
            "      Processed 500/3436 variants...\n",
            "      Processed 1000/3436 variants...\n",
            "      Processed 1500/3436 variants...\n",
            "      Processed 2000/3436 variants...\n",
            "      Processed 2500/3436 variants...\n",
            "      Processed 3000/3436 variants...\n",
            "\n",
            "   PROCESSING SUMMARY:\n",
            "      Total variants: 3436\n",
            "      Variants with LitVar PMIDs: 2825\n",
            "      Perfect citation matches: 2045\n",
            "      Partial citation matches: 1389\n",
            "      No citation matches: 0\n",
            "\n",
            "   Completed processing 3436 variants\n",
            "\n",
            " SELECTING TOP 10 REPRESENTATIVE VARIANTS\n",
            "   Selected: Variant 17677 (Score: 1809.0)\n",
            "      Change: Pathogenic → Unknown\n",
            "      Publications: 452\n",
            "      Citations causing change: 18\n",
            "   Selected: Variant 17661 (Score: 1652.5)\n",
            "      Change: Conflicting interpretations → Unknown\n",
            "      Publications: 524\n",
            "      Citations causing change: 4\n",
            "   Selected: Variant 54360 (Score: 1567.5)\n",
            "      Change: Conflicting interpretations → Likely pathogenic\n",
            "      Publications: 522\n",
            "      Citations causing change: 1\n",
            "   Selected: Variant 37613 (Score: 1550.5)\n",
            "      Change: Conflicting interpretations → Likely benign\n",
            "      Publications: 329\n",
            "      Citations causing change: 16\n",
            "   Selected: Variant 54618 (Score: 1368.5)\n",
            "      Change: Conflicting interpretations → Likely benign\n",
            "      Publications: 368\n",
            "      Citations causing change: 4\n",
            "   Selected: Variant 37421 (Score: 1350.5)\n",
            "      Change: Conflicting interpretations → Likely pathogenic\n",
            "      Publications: 90\n",
            "      Citations causing change: 35\n",
            "   Selected: Variant 41812 (Score: 1305.5)\n",
            "      Change: Conflicting interpretations → Unknown\n",
            "      Publications: 367\n",
            "      Citations causing change: 2\n",
            "   Selected: Variant 91594 (Score: 1300.5)\n",
            "      Change: Conflicting interpretations → Uncertain significance\n",
            "      Publications: 367\n",
            "      Citations causing change: 2\n",
            "   Selected: Variant 55397 (Score: 1109.5)\n",
            "      Change: Conflicting interpretations → Likely pathogenic\n",
            "      Publications: 164\n",
            "      Citations causing change: 18\n",
            "   Selected: Variant 531339 (Score: 1101.5)\n",
            "      Change: Conflicting interpretations → Benign\n",
            "      Publications: 282\n",
            "      Citations causing change: 3\n",
            "\n",
            " CREATING TIMELINE SCATTER VISUALIZATION\n",
            "   Saved timeline scatter plot visualization\n",
            "\n",
            " GENERATING ML FEATURES\n",
            "   Generated 3436 feature rows with 14 features\n",
            "   Saved ML features for 3436 variants\n",
            "\n",
            "================================================================================\n",
            "LITVAR ANALYSIS REPORT - JOURNAL IMPACT FACTOR TIMELINE\n",
            "================================================================================\n",
            "Analysis Date: 2025-08-17 14:32:36\n",
            "\n",
            "OVERALL STATISTICS:\n",
            "  Total variants analyzed: 3436\n",
            "  Total publications found: 44739\n",
            "  Publications causing classification changes: 10912\n",
            "\n",
            "TOP 10 REPRESENTATIVE VARIANTS:\n",
            "\n",
            "  1. Variant 17677 (rs80357906):\n",
            "     Classification: Pathogenic → Unknown\n",
            "     Total publications: 452\n",
            "     Publications causing change: 18\n",
            "     Key publications causing change:\n",
            "       - PMID 7894492: Nature genetics (IF: 53.8)\n",
            "       - PMID 8644703: American journal of human genetics (IF: 17.2)\n",
            "       - PMID 10788334: American journal of human genetics (IF: 17.2)\n",
            "\n",
            "  2. Variant 17661 (rs28897672):\n",
            "     Classification: Conflicting interpretations → Unknown\n",
            "     Total publications: 524\n",
            "     Publications causing change: 4\n",
            "     Key publications causing change:\n",
            "       - PMID 10788334: American journal of human genetics (IF: 17.2)\n",
            "       - PMID 7795652: Nature genetics (IF: 53.8)\n",
            "       - PMID 15024741: Human mutation (IF: 7.9)\n",
            "\n",
            "  3. Variant 54360 (rs28897672):\n",
            "     Classification: Conflicting interpretations → Likely pathogenic\n",
            "     Total publications: 522\n",
            "     Publications causing change: 1\n",
            "     Key publications causing change:\n",
            "       - PMID 26467025: Human mutation (IF: 7.9)\n",
            "\n",
            "  4. Variant 37613 (rs1799966):\n",
            "     Classification: Conflicting interpretations → Likely benign\n",
            "     Total publications: 329\n",
            "     Publications causing change: 16\n",
            "     Key publications causing change:\n",
            "       - PMID 26183948: BMC cancer (IF: 6.8)\n",
            "       - PMID 26092435: Human genomics (IF: 8.8)\n",
            "       - PMID 21447777: Cancer epidemiology, biomarkers & prevention : a publication of the American Association for Cancer Research, cosponsored by the American Society of Preventive Oncology (IF: 2.0)\n",
            "\n",
            "  5. Variant 54618 (rs799917):\n",
            "     Classification: Conflicting interpretations → Likely benign\n",
            "     Total publications: 368\n",
            "     Publications causing change: 4\n",
            "     Key publications causing change:\n",
            "       - PMID 24244370: PloS one (IF: 6.0)\n",
            "       - PMID 31911673: Genetics in medicine : official journal of the American College of Medical Genetics (IF: 4.0)\n",
            "       - PMID 25896959: Clinica chimica acta; international journal of clinical chemistry (IF: 3.0)\n",
            "\n",
            "  6. Variant 37421 (rs80357084):\n",
            "     Classification: Conflicting interpretations → Likely pathogenic\n",
            "     Total publications: 90\n",
            "     Publications causing change: 35\n",
            "     Key publications causing change:\n",
            "       - PMID 28440963: Cancer science (IF: 10.4)\n",
            "       - PMID 25802882: Molecular genetics & genomic medicine (IF: 4.4)\n",
            "       - PMID 29215753: Cancer science (IF: 10.4)\n",
            "\n",
            "  7. Variant 41812 (rs799917):\n",
            "     Classification: Conflicting interpretations → Unknown\n",
            "     Total publications: 367\n",
            "     Publications causing change: 2\n",
            "     Key publications causing change:\n",
            "       - PMID 24728327: PloS one (IF: 6.0)\n",
            "       - PMID 22703879: American journal of human genetics (IF: 17.2)\n",
            "\n",
            "  8. Variant 91594 (rs799917):\n",
            "     Classification: Conflicting interpretations → Uncertain significance\n",
            "     Total publications: 367\n",
            "     Publications causing change: 2\n",
            "     Key publications causing change:\n",
            "       - PMID 31911673: Genetics in medicine : official journal of the American College of Medical Genetics (IF: 4.0)\n",
            "       - PMID 25741868: Genetics in medicine : official journal of the American College of Medical Genetics (IF: 4.0)\n",
            "\n",
            "  9. Variant 55397 (rs41293459):\n",
            "     Classification: Conflicting interpretations → Likely pathogenic\n",
            "     Total publications: 164\n",
            "     Publications causing change: 18\n",
            "     Key publications causing change:\n",
            "       - PMID 20455026: Familial cancer (IF: 5.1)\n",
            "       - PMID 24504028: Scientific reports (IF: 7.5)\n",
            "       - PMID 11157798: Human molecular genetics (IF: 8.9)\n",
            "\n",
            "  10. Variant 531339 (rs16942):\n",
            "     Classification: Conflicting interpretations → Benign\n",
            "     Total publications: 282\n",
            "     Publications causing change: 3\n",
            "     Key publications causing change:\n",
            "       - PMID 31911673: Genetics in medicine : official journal of the American College of Medical Genetics (IF: 4.0)\n",
            "       - PMID 25741868: Genetics in medicine : official journal of the American College of Medical Genetics (IF: 4.0)\n",
            "       - PMID 25741868: Genetics in medicine : official journal of the American College of Medical Genetics (IF: 4.0)\n",
            "\n",
            "JOURNAL IMPACT FACTOR DISTRIBUTION:\n",
            "  Average impact factor: 13.32\n",
            "  Median impact factor: 6.00\n",
            "  Max impact factor: 642.90\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS COMPLETE\n",
            "================================================================================\n",
            "Total runtime: 63.6 seconds (1.1 minutes)\n",
            "Results saved to: ./litvar_analysis_results\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict, Counter\n",
        "import json\n",
        "import time\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from typing import List, Dict, Tuple, Optional, Set\n",
        "import logging\n",
        "import concurrent.futures\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class EnhancedLitVarAnalyzer:\n",
        "\n",
        "    def __init__(self, email: str = \"yl8889@nyu.edu\", api_key: str = None,\n",
        "                 impact_factor_csv: str = None):\n",
        "        self.email = email\n",
        "        self.api_key = api_key\n",
        "\n",
        "        # API endpoints\n",
        "        self.litvar_sensor = \"https://www.ncbi.nlm.nih.gov/research/litvar2-api/sensor\"\n",
        "        self.pubmed_api = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
        "\n",
        "        # Cache for API responses\n",
        "        self.cache = {}\n",
        "        self.cache_dir = \"./litvar_cache\"\n",
        "        os.makedirs(self.cache_dir, exist_ok=True)\n",
        "\n",
        "        # Performance optimization settings\n",
        "        self.max_workers = 20  # Increased for parallel processing\n",
        "        self.batch_size = 100  # For batch API calls\n",
        "        self.use_disk_cache = True  # Enable disk caching\n",
        "        self.cache_ttl = 7 * 24 * 3600  # Cache for 7 days\n",
        "\n",
        "        self.all_variant_data = {}\n",
        "\n",
        "        # Load journal impact factors from CSV if provided\n",
        "        self.journal_impact_factors = self.load_impact_factors(impact_factor_csv)\n",
        "\n",
        "        # Session for connection pooling\n",
        "        self.session = requests.Session()\n",
        "        self.session.mount('https://', requests.adapters.HTTPAdapter(\n",
        "            pool_connections=100,\n",
        "            pool_maxsize=100,\n",
        "            max_retries=3\n",
        "        ))\n",
        "\n",
        "    def load_impact_factors(self, csv_path: str = None) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Load journal impact factors from CSV file using CiteScore column.\n",
        "        Returns a dictionary mapping journal names to impact factors.\n",
        "        \"\"\"\n",
        "        if not csv_path or not os.path.exists(csv_path):\n",
        "            print(\"No impact factor CSV provided or file not found, using defaults\")\n",
        "            return {\n",
        "                'default': 2.0,\n",
        "                'high_impact': 10.0\n",
        "            }\n",
        "\n",
        "        print(f\"Loading journal CiteScores from {csv_path}\")\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path)\n",
        "\n",
        "            impact_dict = {}\n",
        "            if 'CiteScore' in df.columns and 'Title' in df.columns:\n",
        "                for _, row in df.iterrows():\n",
        "                    journal_name = str(row['Title']).strip()\n",
        "                    try:\n",
        "                        citescore = float(row['CiteScore'])\n",
        "                        if pd.notna(citescore) and citescore > 0:\n",
        "                            impact_dict[journal_name.lower()] = citescore\n",
        "\n",
        "                            if journal_name.lower() == 'ca-a cancer journal for clinicians':\n",
        "                                impact_dict['ca cancer j clin'] = citescore\n",
        "                                impact_dict['ca-a cancer j clin'] = citescore\n",
        "                            elif journal_name.lower() == 'new england journal of medicine':\n",
        "                                impact_dict['nejm'] = citescore\n",
        "                                impact_dict['n engl j med'] = citescore\n",
        "                                impact_dict['the new england journal of medicine'] = citescore\n",
        "                            elif journal_name.lower() == 'nature':\n",
        "                                impact_dict['nature'] = citescore\n",
        "                            elif journal_name.lower() == 'cell':\n",
        "                                impact_dict['cell'] = citescore\n",
        "                            elif journal_name.lower() == 'science':\n",
        "                                impact_dict['science'] = citescore\n",
        "\n",
        "                    except (ValueError, TypeError):\n",
        "                        continue\n",
        "\n",
        "                print(f\"   Loaded CiteScores for {len(impact_dict)} journals\")\n",
        "\n",
        "                sorted_items = sorted(impact_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "                print(f\"   Highest CiteScores loaded:\")\n",
        "                for journal, score in sorted_items[:5]:\n",
        "                    print(f\"      {journal[:50]}: {score:.1f}\")\n",
        "            else:\n",
        "                print(f\"   Required columns (Title, CiteScore) not found in CSV\")\n",
        "                print(f\"   Available columns: {list(df.columns)[:10]}\")\n",
        "\n",
        "            return impact_dict if impact_dict else {'default': 2.0}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   Error loading CiteScores: {str(e)}\")\n",
        "            return {'default': 2.0}\n",
        "\n",
        "    def load_json_classification_changes(self, filepath: str) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load and preprocess classification changes from JSON file.\n",
        "        \"\"\"\n",
        "        print(f\"\\n LOADING CLASSIFICATION CHANGES FROM JSON\")\n",
        "        print(f\"   File: {filepath}\")\n",
        "\n",
        "        with open(filepath, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        variants_list = []\n",
        "\n",
        "        for variant in data.get('variants', []):\n",
        "            if 'classification_changes' in variant and variant['classification_changes']:\n",
        "                for change in variant['classification_changes']:\n",
        "                    # Extract citation PMIDs that caused the change\n",
        "                    citation_pmids = []\n",
        "                    for citation in change.get('citations_new', []):\n",
        "                        if 'pmid' in citation:\n",
        "                            citation_pmids.append(citation['pmid'])\n",
        "\n",
        "                    variant_record = {\n",
        "                        'VariationID': variant['variation_id'],\n",
        "                        'VCV': variant['vcv_accession'],\n",
        "                        'RS# (dbSNP)_new': variant.get('rsid', ''),\n",
        "                        'Gene': variant.get('gene', 'BRCA1'),\n",
        "                        'Submitter': change.get('submitter', ''),\n",
        "                        'date_old': change.get('date_old', ''),\n",
        "                        'ClinicalSignificance_old': change.get('classification_old', ''),\n",
        "                        'ClinicalSignificance_old_norm': self.normalize_classification(\n",
        "                            change.get('classification_old', '')\n",
        "                        ),\n",
        "                        'date_new': change.get('date_new', ''),\n",
        "                        'ClinicalSignificance_new': change.get('classification_new', ''),\n",
        "                        'ClinicalSignificance_new_norm': self.normalize_classification(\n",
        "                            change.get('classification_new', '')\n",
        "                        ),\n",
        "                        'SCV_old': change.get('scv_old', ''),\n",
        "                        'SCV_new': change.get('scv_new', ''),\n",
        "                        'change_type': change.get('change_type', ''),\n",
        "                        'citation_pmids': citation_pmids,\n",
        "                        'citation_count': len(citation_pmids)\n",
        "                    }\n",
        "                    variants_list.append(variant_record)\n",
        "\n",
        "        df = pd.DataFrame(variants_list)\n",
        "\n",
        "        for col in ['date_old', 'date_new']:\n",
        "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "\n",
        "        # Remove rows with missing critical data\n",
        "        df = df.dropna(subset=['VariationID', 'ClinicalSignificance_new', 'date_new'])\n",
        "\n",
        "        print(f\"   Loaded {len(df)} classification changes\")\n",
        "        print(f\"   Found {df['citation_count'].sum()} total citations\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def normalize_classification(self, classification: str) -> str:\n",
        "        \"\"\"\n",
        "        Normalize classification names.\n",
        "        \"\"\"\n",
        "        if not classification:\n",
        "            return 'Unknown'\n",
        "\n",
        "        classification = classification.lower().strip()\n",
        "\n",
        "        if 'pathogenic' in classification and 'likely' in classification:\n",
        "            return 'Likely pathogenic'\n",
        "        elif 'pathogenic' in classification:\n",
        "            return 'Pathogenic'\n",
        "        elif 'benign' in classification and 'likely' in classification:\n",
        "            return 'Likely benign'\n",
        "        elif 'benign' in classification:\n",
        "            return 'Benign'\n",
        "        elif 'uncertain' in classification or 'unknown' in classification:\n",
        "            return 'Uncertain significance'\n",
        "        elif 'conflict' in classification or 'multiple' in classification:\n",
        "            return 'Conflicting interpretations'\n",
        "        else:\n",
        "            return 'Unknown'\n",
        "\n",
        "    def fetch_publication_details(self, pmid: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Fetch publication details from PubMed including journal and publication date.\n",
        "        \"\"\"\n",
        "        cache_key = f\"pubmed_{pmid}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        result = {\n",
        "            'pmid': pmid,\n",
        "            'title': '',\n",
        "            'journal': '',\n",
        "            'publication_date': None,\n",
        "            'impact_factor': 1.0,  # Default impact factor\n",
        "            'authors': [],\n",
        "            'abstract': ''\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Fetch from PubMed\n",
        "            url = f\"{self.pubmed_api}/esummary.fcgi\"\n",
        "            params = {\n",
        "                'db': 'pubmed',\n",
        "                'id': pmid,\n",
        "                'retmode': 'json',\n",
        "                'email': self.email\n",
        "            }\n",
        "\n",
        "            if self.api_key:\n",
        "                params['api_key'] = self.api_key\n",
        "\n",
        "            response = requests.get(url, params=params, timeout=10)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                if 'result' in data and pmid in data['result']:\n",
        "                    pub_data = data['result'][pmid]\n",
        "\n",
        "                    result['title'] = pub_data.get('title', '')\n",
        "                    result['journal'] = pub_data.get('fulljournalname', pub_data.get('source', ''))\n",
        "\n",
        "                    # Parse publication date\n",
        "                    pub_date = pub_data.get('pubdate', pub_data.get('epubdate', ''))\n",
        "                    if pub_date:\n",
        "                        try:\n",
        "                            result['publication_date'] = pd.to_datetime(pub_date, errors='coerce')\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                    # Get authors\n",
        "                    if 'authors' in pub_data:\n",
        "                        result['authors'] = [author.get('name', '') for author in pub_data['authors']]\n",
        "\n",
        "                    # Assign impact factor based on journal\n",
        "                    impact_assigned = False\n",
        "                    journal_lower = result['journal'].lower()\n",
        "\n",
        "                    # Try exact match first\n",
        "                    if journal_lower in self.journal_impact_factors:\n",
        "                        result['impact_factor'] = self.journal_impact_factors[journal_lower]\n",
        "                        impact_assigned = True\n",
        "                    else:\n",
        "                        # Try partial matches\n",
        "                        for journal_key, impact in self.journal_impact_factors.items():\n",
        "                            if journal_key in journal_lower or journal_lower in journal_key:\n",
        "                                result['impact_factor'] = impact\n",
        "                                impact_assigned = True\n",
        "                                break\n",
        "\n",
        "                        # Try matching key words\n",
        "                        if not impact_assigned:\n",
        "                            journal_words = set(journal_lower.split())\n",
        "                            for journal_key, impact in self.journal_impact_factors.items():\n",
        "                                key_words = set(journal_key.split())\n",
        "                                if len(journal_words & key_words) >= 2:\n",
        "                                    result['impact_factor'] = impact\n",
        "                                    impact_assigned = True\n",
        "                                    break\n",
        "\n",
        "                    # If no match found, assign based on journal type\n",
        "                    if not impact_assigned:\n",
        "                        if 'nature' in journal_lower:\n",
        "                            result['impact_factor'] = 15.0\n",
        "                        elif 'science' in journal_lower:\n",
        "                            result['impact_factor'] = 12.0\n",
        "                        elif 'cell' in journal_lower:\n",
        "                            result['impact_factor'] = 10.0\n",
        "                        elif 'genetics' in journal_lower or 'genomics' in journal_lower:\n",
        "                            result['impact_factor'] = 5.0\n",
        "                        elif 'cancer' in journal_lower or 'oncol' in journal_lower:\n",
        "                            result['impact_factor'] = 6.0\n",
        "                        elif 'medicine' in journal_lower or 'medical' in journal_lower:\n",
        "                            result['impact_factor'] = 4.0\n",
        "                        elif 'clinical' in journal_lower:\n",
        "                            result['impact_factor'] = 3.5\n",
        "                        elif 'molecular' in journal_lower:\n",
        "                            result['impact_factor'] = 4.5\n",
        "                        elif 'journal' in journal_lower:\n",
        "                            result['impact_factor'] = 2.5\n",
        "                        else:\n",
        "                            result['impact_factor'] = 2.0  # Default for unknown journals\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"         Error fetching PubMed data for {pmid}: {str(e)[:100]}\")\n",
        "\n",
        "        self.cache[cache_key] = result\n",
        "        time.sleep(0.2)  # Rate limiting\n",
        "\n",
        "        return result\n",
        "\n",
        "    def fetch_litvar_publications_with_details(self, rsid: str, citation_pmids: List[str] = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Fetch publications from LitVar2 API for a given rsID.\n",
        "        \"\"\"\n",
        "        print(f\"      Fetching LitVar2 publications for rs{rsid}\")\n",
        "\n",
        "        if not rsid.startswith('rs'):\n",
        "            rsid = f'rs{rsid}'\n",
        "\n",
        "        result = {\n",
        "            'rsid': rsid,\n",
        "            'pmids': [],\n",
        "            'publications': [],\n",
        "            'citation_publications': [],  # Publications that caused classification changes\n",
        "            'other_publications': []  # Other related publications\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            api_url = f\"https://www.ncbi.nlm.nih.gov/research/litvar2-api/variant/get/litvar@{rsid}%23%23/publications\"\n",
        "\n",
        "            print(f\"         Calling LitVar2 API: {api_url}\")\n",
        "\n",
        "            # Make GET request with proper headers\n",
        "            headers = {\n",
        "                'Accept': 'application/json'\n",
        "            }\n",
        "\n",
        "            response = requests.get(\n",
        "                api_url,\n",
        "                headers=headers,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            print(f\"         Response status: {response.status_code}\")\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                if isinstance(data, dict) and 'pmids' in data:\n",
        "                    pmids = data['pmids']\n",
        "                    result['pmids'] = [str(p) for p in pmids]\n",
        "                    print(f\"         Retrieved {len(result['pmids'])} PMIDs from LitVar2\")\n",
        "                    print(f\"         PMIDs: {result['pmids'][:10]}...\")  # Show first 10\n",
        "                elif isinstance(data, list):\n",
        "                    # Sometimes it might return a list directly\n",
        "                    result['pmids'] = [str(p) for p in data if p]\n",
        "                    print(f\"         Retrieved {len(result['pmids'])} PMIDs from LitVar2 (list format)\")\n",
        "                else:\n",
        "                    print(f\"         Unexpected response format: {type(data)}\")\n",
        "                    print(f\"         Response preview: {str(data)[:200]}\")\n",
        "            else:\n",
        "                print(f\"         API returned status {response.status_code}\")\n",
        "                print(f\"         Response: {response.text[:500]}\")\n",
        "\n",
        "                # Try alternative LitVar2 endpoint without the @ symbol\n",
        "                alt_url = f\"https://www.ncbi.nlm.nih.gov/research/litvar2-api/variant/get/litvar{rsid}%23%23/publications\"\n",
        "                print(f\"         Trying alternative format: {alt_url}\")\n",
        "\n",
        "                alt_response = requests.get(alt_url, headers=headers, timeout=30)\n",
        "                if alt_response.status_code == 200:\n",
        "                    alt_data = alt_response.json()\n",
        "                    if isinstance(alt_data, dict) and 'pmids' in alt_data:\n",
        "                        result['pmids'] = [str(p) for p in alt_data['pmids']]\n",
        "                        print(f\"         Retrieved {len(result['pmids'])} PMIDs with alternative format\")\n",
        "\n",
        "            # Also check the sensor endpoint for comparison\n",
        "            sensor_url = f\"https://www.ncbi.nlm.nih.gov/research/litvar2-api/sensor/{rsid}\"\n",
        "            try:\n",
        "                sensor_response = requests.get(sensor_url, timeout=10)\n",
        "                if sensor_response.status_code == 200:\n",
        "                    sensor_data = sensor_response.json()\n",
        "                    expected_count = sensor_data.get('pmids_count', 0)\n",
        "                    if expected_count > 0:\n",
        "                        actual_count = len(result['pmids'])\n",
        "                        if actual_count > 0:\n",
        "                            print(f\"         Sensor shows {expected_count} publications, retrieved {actual_count} PMIDs\")\n",
        "                            if actual_count < expected_count:\n",
        "                                print(f\"          {expected_count - actual_count} PMIDs may be missing\")\n",
        "                        else:\n",
        "                            print(f\"         Sensor shows {expected_count} publications but couldn't retrieve PMIDs\")\n",
        "            except Exception as e:\n",
        "                print(f\"         Sensor check failed: {str(e)[:50]}\")\n",
        "\n",
        "            fetched_count = 0\n",
        "            citations_matched = 0\n",
        "\n",
        "            if result['pmids']:\n",
        "                print(f\"         Processing {len(result['pmids'][:100])} PMIDs for publication details...\")\n",
        "\n",
        "                for pmid in result['pmids'][:100]:  # Limit to first 100 for performance\n",
        "                    # Clean the PMID\n",
        "                    clean_pmid = str(pmid).strip()\n",
        "\n",
        "                    # Fetch publication details from PubMed\n",
        "                    pub_details = self.fetch_publication_details(clean_pmid)\n",
        "\n",
        "                    if pub_details and (pub_details.get('journal') or pub_details.get('title')):\n",
        "                        fetched_count += 1\n",
        "\n",
        "                        # Check if this publication caused a classification change\n",
        "                        # by comparing with citation_pmids from the JSON input\n",
        "                        if citation_pmids:\n",
        "                            # Normalize PMIDs for comparison\n",
        "                            clean_citations = [str(p).strip() for p in citation_pmids]\n",
        "\n",
        "                            if clean_pmid in clean_citations:\n",
        "                                pub_details['caused_change'] = True\n",
        "                                result['citation_publications'].append(pub_details)\n",
        "                                citations_matched += 1\n",
        "                                print(f\"         PMID {clean_pmid} caused classification change\")\n",
        "                            else:\n",
        "                                pub_details['caused_change'] = False\n",
        "                                result['other_publications'].append(pub_details)\n",
        "                        else:\n",
        "                            pub_details['caused_change'] = False\n",
        "                            result['other_publications'].append(pub_details)\n",
        "\n",
        "                        result['publications'].append(pub_details)\n",
        "\n",
        "                print(f\"         Fetched details for {fetched_count} publications\")\n",
        "                if citation_pmids:\n",
        "                    print(f\"         {citations_matched} caused classification changes (matched from JSON)\")\n",
        "                    if citations_matched == 0 and len(citation_pmids) > 0:\n",
        "                        print(f\"         No matches found. JSON PMIDs: {citation_pmids[:3]}\")\n",
        "                        print(f\"         LitVar2 PMIDs (first 3): {result['pmids'][:3] if result['pmids'] else 'None'}\")\n",
        "            else:\n",
        "                print(f\"         No PMIDs retrieved for {rsid}\")\n",
        "\n",
        "                # If no PMIDs from LitVar2, still process the citation PMIDs from JSON\n",
        "                if citation_pmids:\n",
        "                    print(f\"         Processing citation PMIDs from JSON directly...\")\n",
        "                    for pmid in citation_pmids:\n",
        "                        clean_pmid = str(pmid).strip()\n",
        "                        pub_details = self.fetch_publication_details(clean_pmid)\n",
        "                        if pub_details and (pub_details.get('journal') or pub_details.get('title')):\n",
        "                            pub_details['caused_change'] = True\n",
        "                            pub_details['source'] = 'ClinVar_citation_only'\n",
        "                            result['citation_publications'].append(pub_details)\n",
        "                            result['publications'].append(pub_details)\n",
        "                            fetched_count += 1\n",
        "                            citations_matched += 1\n",
        "                            print(f\"         Added citation PMID {clean_pmid} from PubMed (not in LitVar2)\")\n",
        "\n",
        "                    print(f\"         Processed {fetched_count} citation PMIDs from JSON\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"         Error fetching LitVar2 data: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "        return result\n",
        "\n",
        "    def load_cached_data(self, cache_key: str):\n",
        "        \"\"\"Load data from disk cache if available and not expired.\"\"\"\n",
        "        if not self.use_disk_cache:\n",
        "            return None\n",
        "\n",
        "        cache_file = os.path.join(self.cache_dir, f\"{cache_key}.json\")\n",
        "        if os.path.exists(cache_file):\n",
        "            # Check if cache is expired\n",
        "            file_age = time.time() - os.path.getmtime(cache_file)\n",
        "            if file_age < self.cache_ttl:\n",
        "                try:\n",
        "                    with open(cache_file, 'r') as f:\n",
        "                        return json.load(f)\n",
        "                except:\n",
        "                    pass\n",
        "        return None\n",
        "\n",
        "    def save_to_cache(self, cache_key: str, data):\n",
        "        \"\"\"Save data to disk cache.\"\"\"\n",
        "        if not self.use_disk_cache:\n",
        "            return\n",
        "\n",
        "        cache_file = os.path.join(self.cache_dir, f\"{cache_key}.json\")\n",
        "        try:\n",
        "            with open(cache_file, 'w') as f:\n",
        "                json.dump(data, f)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def fetch_litvar_batch(self, rsids: List[str]) -> Dict[str, List[str]]:\n",
        "        \"\"\"\n",
        "        Fetch PMIDs for multiple rsIDs in batches.\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "\n",
        "        # Check cache first\n",
        "        uncached_rsids = []\n",
        "        for rsid in rsids:\n",
        "            cache_key = f\"litvar2_{rsid}\"\n",
        "            cached = self.load_cached_data(cache_key)\n",
        "            if cached:\n",
        "                results[rsid] = cached\n",
        "            else:\n",
        "                uncached_rsids.append(rsid)\n",
        "\n",
        "        if not uncached_rsids:\n",
        "            print(f\"         All {len(rsids)} variants found in cache\")\n",
        "            return results\n",
        "\n",
        "        print(f\"         Found {len(results)} cached, fetching {len(uncached_rsids)} from LitVar2...\")\n",
        "\n",
        "        chunk_size = 20\n",
        "        total_chunks = (len(uncached_rsids) + chunk_size - 1) // chunk_size\n",
        "\n",
        "        for chunk_idx, i in enumerate(range(0, len(uncached_rsids), chunk_size)):\n",
        "            chunk = uncached_rsids[i:i+chunk_size]\n",
        "            print(f\"         Processing chunk {chunk_idx + 1}/{total_chunks} ({len(chunk)} variants)...\")\n",
        "\n",
        "            # Use ThreadPoolExecutor with limited workers\n",
        "            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "                futures = {\n",
        "                    executor.submit(self.fetch_single_litvar, rsid): rsid\n",
        "                    for rsid in chunk\n",
        "                }\n",
        "\n",
        "                for future in concurrent.futures.as_completed(futures):\n",
        "                    rsid = futures[future]\n",
        "                    try:\n",
        "                        pmids = future.result()\n",
        "                        results[rsid] = pmids\n",
        "                        # Cache the result\n",
        "                        self.save_to_cache(f\"litvar2_{rsid}\", pmids)\n",
        "                    except Exception as e:\n",
        "                        print(f\"            Failed for {rsid}: {str(e)[:50]}\")\n",
        "                        results[rsid] = []\n",
        "\n",
        "            # avoid rate limiting\n",
        "            if chunk_idx < total_chunks - 1:\n",
        "                time.sleep(0.5)\n",
        "\n",
        "        print(f\"         Fetched data for {len(results)} variants total\")\n",
        "        return results\n",
        "\n",
        "    def fetch_single_litvar(self, rsid: str) -> List[str]:\n",
        "        \"\"\"Fetch PMIDs for a single rsID from LitVar2 with retry logic.\"\"\"\n",
        "        if not rsid.startswith('rs'):\n",
        "            rsid = f'rs{rsid}'\n",
        "\n",
        "        api_url = f\"https://www.ncbi.nlm.nih.gov/research/litvar2-api/variant/get/litvar@{rsid}%23%23/publications\"\n",
        "\n",
        "        max_retries = 3\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = self.session.get(api_url, timeout=10)\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "                    if isinstance(data, dict) and 'pmids' in data:\n",
        "                        return [str(p) for p in data['pmids']]\n",
        "                elif response.status_code == 429:  # Rate limited\n",
        "                    print(f\"            Rate limited, waiting...\")\n",
        "                    time.sleep(2 ** attempt)\n",
        "                    continue\n",
        "                return []\n",
        "            except requests.exceptions.ConnectionError as e:\n",
        "                if attempt < max_retries - 1:\n",
        "                    print(f\"            Connection error for {rsid}, retrying...\")\n",
        "                    time.sleep(1)\n",
        "                    continue\n",
        "                return []\n",
        "            except Exception:\n",
        "                return []\n",
        "        return []\n",
        "\n",
        "    def fetch_pubmed_batch(self, pmids: List[str]) -> Dict[str, Dict]:\n",
        "        \"\"\"\n",
        "        Fetch details for multiple PMIDs in batch from PubMed.\n",
        "        Much more efficient than individual requests.\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "\n",
        "        # Check cache first\n",
        "        uncached_pmids = []\n",
        "        for pmid in pmids:\n",
        "            cache_key = f\"pubmed_{pmid}\"\n",
        "            if cache_key in self.cache:\n",
        "                results[pmid] = self.cache[cache_key]\n",
        "            else:\n",
        "                cached = self.load_cached_data(cache_key)\n",
        "                if cached:\n",
        "                    results[pmid] = cached\n",
        "                    self.cache[cache_key] = cached\n",
        "                else:\n",
        "                    uncached_pmids.append(pmid)\n",
        "\n",
        "        if not uncached_pmids:\n",
        "            return results\n",
        "\n",
        "        # Batch fetch from PubMed\n",
        "        print(f\"         Fetching batch of {len(uncached_pmids)} publications from PubMed...\")\n",
        "\n",
        "        # PubMed allows up to 200 IDs per request\n",
        "        chunk_size = 200\n",
        "        for i in range(0, len(uncached_pmids), chunk_size):\n",
        "            chunk = uncached_pmids[i:i+chunk_size]\n",
        "\n",
        "            url = f\"{self.pubmed_api}/esummary.fcgi\"\n",
        "            params = {\n",
        "                'db': 'pubmed',\n",
        "                'id': ','.join(chunk),\n",
        "                'retmode': 'json',\n",
        "                'email': self.email\n",
        "            }\n",
        "\n",
        "            if self.api_key:\n",
        "                params['api_key'] = self.api_key\n",
        "\n",
        "            try:\n",
        "                response = self.session.get(url, params=params, timeout=30)\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "                    if 'result' in data:\n",
        "                        for pmid in chunk:\n",
        "                            if pmid in data['result']:\n",
        "                                pub_data = data['result'][pmid]\n",
        "                                result = self.parse_pubmed_data(pub_data, pmid)\n",
        "                                results[pmid] = result\n",
        "                                # Cache the result\n",
        "                                self.cache[f\"pubmed_{pmid}\"] = result\n",
        "                                self.save_to_cache(f\"pubmed_{pmid}\", result)\n",
        "            except Exception as e:\n",
        "                print(f\"         Batch PubMed fetch failed: {str(e)[:50]}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def parse_pubmed_data(self, pub_data: Dict, pmid: str) -> Dict:\n",
        "        \"\"\"Parse PubMed data into our format with STRICT journal matching.\"\"\"\n",
        "        result = {\n",
        "            'pmid': pmid,\n",
        "            'title': pub_data.get('title', ''),\n",
        "            'journal': pub_data.get('fulljournalname', pub_data.get('source', '')),\n",
        "            'publication_date': None,\n",
        "            'impact_factor': 2.0,  # Default impact factor\n",
        "            'authors': [],\n",
        "            'abstract': ''\n",
        "        }\n",
        "\n",
        "        pub_date = pub_data.get('pubdate', pub_data.get('epubdate', ''))\n",
        "        if pub_date:\n",
        "            try:\n",
        "                result['publication_date'] = pd.to_datetime(pub_date, errors='coerce')\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        if 'authors' in pub_data:\n",
        "            result['authors'] = [author.get('name', '') for author in pub_data['authors']]\n",
        "\n",
        "        journal_name = result['journal']\n",
        "        journal_lower = journal_name.lower().strip()\n",
        "\n",
        "        # First try exact match\n",
        "        if journal_lower in self.journal_impact_factors:\n",
        "            result['impact_factor'] = self.journal_impact_factors[journal_lower]\n",
        "            # debug\n",
        "            if self.journal_impact_factors[journal_lower] > 600:\n",
        "                print(f\"         High CiteScore {self.journal_impact_factors[journal_lower]:.1f} for: {journal_name}\")\n",
        "        else:\n",
        "            # For non-exact matches\n",
        "            # Only match if it's a clear abbreviation or variant\n",
        "            matched = False\n",
        "\n",
        "            # Check for known abbreviations\n",
        "            if 'ca cancer j clin' in journal_lower or 'ca-a cancer j clin' in journal_lower:\n",
        "                if 'ca cancer j clin' in self.journal_impact_factors:\n",
        "                    result['impact_factor'] = self.journal_impact_factors['ca cancer j clin']\n",
        "                    matched = True\n",
        "            elif 'n engl j med' in journal_lower or 'nejm' in journal_lower:\n",
        "                if 'new england journal of medicine' in self.journal_impact_factors:\n",
        "                    result['impact_factor'] = self.journal_impact_factors['new england journal of medicine']\n",
        "                    matched = True\n",
        "\n",
        "            # If still no match, use conservative defaults based on journal type\n",
        "            if not matched:\n",
        "                # DO NOT do partial string matching that could incorrectly assign high scores\n",
        "                if 'nature' == journal_lower.split()[0] if journal_lower.split() else '':\n",
        "                    result['impact_factor'] = 15.0\n",
        "                elif 'science' == journal_lower:\n",
        "                    result['impact_factor'] = 12.0\n",
        "                elif 'cell' == journal_lower:\n",
        "                    result['impact_factor'] = 10.0\n",
        "                elif 'cancer' in journal_lower and 'ca' not in journal_lower[:5]:  # Avoid CA journal\n",
        "                    result['impact_factor'] = 5.0\n",
        "                elif 'genetics' in journal_lower or 'genomics' in journal_lower:\n",
        "                    result['impact_factor'] = 4.0\n",
        "                elif 'medicine' in journal_lower or 'medical' in journal_lower:\n",
        "                    result['impact_factor'] = 3.5\n",
        "                elif 'clinical' in journal_lower:\n",
        "                    result['impact_factor'] = 3.0\n",
        "                else:\n",
        "                    result['impact_factor'] = 2.0  # Conservative default\n",
        "\n",
        "        return result\n",
        "\n",
        "    def process_all_variants_optimized(self, changes_df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"\n",
        "        Optimized version: Process all variants with batch operations and parallel processing.\n",
        "        \"\"\"\n",
        "        print(f\"\\n FETCHING PUBLICATIONS FOR ALL VARIANTS (OPTIMIZED)\")\n",
        "\n",
        "        all_variant_data = {}\n",
        "\n",
        "        # Group by VariationID\n",
        "        grouped = changes_df.groupby('VariationID').agg({\n",
        "            'RS# (dbSNP)_new': 'first',\n",
        "            'date_new': 'max',\n",
        "            'date_old': 'min',\n",
        "            'ClinicalSignificance_old_norm': 'first',\n",
        "            'ClinicalSignificance_new_norm': 'last',\n",
        "            'citation_pmids': lambda x: [pmid for sublist in x for pmid in sublist]\n",
        "        }).reset_index()\n",
        "\n",
        "        print(f\"   Found {len(grouped)} unique variants to process\")\n",
        "\n",
        "        # Filter valid rsIDs\n",
        "        valid_variants = grouped[\n",
        "            (grouped['RS# (dbSNP)_new'].notna()) &\n",
        "            (grouped['RS# (dbSNP)_new'] != 'None')\n",
        "        ].copy()\n",
        "\n",
        "        print(f\"   {len(valid_variants)} variants with valid rsIDs\")\n",
        "\n",
        "        # Batch fetch all LitVar data\n",
        "        print(f\"\\n   BATCH FETCHING FROM LITVAR2...\")\n",
        "        all_rsids = valid_variants['RS# (dbSNP)_new'].tolist()\n",
        "\n",
        "        batch_size = 100  # Process 100 variants at a time\n",
        "        litvar_results = {}\n",
        "\n",
        "        for batch_start in range(0, len(all_rsids), batch_size):\n",
        "            batch_end = min(batch_start + batch_size, len(all_rsids))\n",
        "            batch_rsids = all_rsids[batch_start:batch_end]\n",
        "\n",
        "            print(f\"\\n   Processing variants {batch_start + 1}-{batch_end} of {len(all_rsids)}...\")\n",
        "            batch_results = self.fetch_litvar_batch(batch_rsids)\n",
        "            litvar_results.update(batch_results)\n",
        "\n",
        "            # Show progress\n",
        "            if batch_end < len(all_rsids):\n",
        "                print(f\"   Progress: {batch_end}/{len(all_rsids)} variants ({100*batch_end/len(all_rsids):.1f}%)\")\n",
        "\n",
        "        print(f\"\\n   Completed LitVar fetching for {len(litvar_results)} variants\")\n",
        "\n",
        "        # Collect all unique PMIDs\n",
        "        print(f\"\\n   Collecting unique PMIDs...\")\n",
        "        all_pmids = set()\n",
        "        for pmids in litvar_results.values():\n",
        "            all_pmids.update(pmids)\n",
        "\n",
        "        # Add citation PMIDs\n",
        "        for citation_pmids in valid_variants['citation_pmids']:\n",
        "            if citation_pmids:\n",
        "                all_pmids.update([str(p) for p in citation_pmids])\n",
        "\n",
        "        print(f\"   Found {len(all_pmids)} unique PMIDs total\")\n",
        "\n",
        "        # Batch fetch all PubMed data\n",
        "        print(f\"\\n   BATCH FETCHING PUBLICATIONS FROM PUBMED...\")\n",
        "        pubmed_results = {}\n",
        "\n",
        "        # Process PubMed in batches\n",
        "        pmid_list = list(all_pmids)\n",
        "        pubmed_batch_size = 500  # PubMed can handle larger batches\n",
        "\n",
        "        for batch_start in range(0, len(pmid_list), pubmed_batch_size):\n",
        "            batch_end = min(batch_start + pubmed_batch_size, len(pmid_list))\n",
        "            batch_pmids = pmid_list[batch_start:batch_end]\n",
        "\n",
        "            print(f\"   Fetching PubMed batch {batch_start + 1}-{batch_end} of {len(pmid_list)}...\")\n",
        "            batch_results = self.fetch_pubmed_batch(batch_pmids)\n",
        "            pubmed_results.update(batch_results)\n",
        "\n",
        "        print(f\"   Fetched details for {len(pubmed_results)} publications\")\n",
        "\n",
        "        # Process each variant with the pre-fetched data\n",
        "        print(f\"\\n   PROCESSING VARIANT DATA...\")\n",
        "\n",
        "        for idx, row in valid_variants.iterrows():\n",
        "            variant_id = str(row['VariationID'])\n",
        "            rsid = str(row['RS# (dbSNP)_new'])\n",
        "            citation_pmids = row['citation_pmids'] if row['citation_pmids'] else []\n",
        "\n",
        "            litvar_pmids = litvar_results.get(rsid, [])\n",
        "\n",
        "            publications = []\n",
        "            citation_publications = []\n",
        "            other_publications = []\n",
        "\n",
        "            # Process LitVar publications\n",
        "            for pmid in litvar_pmids:\n",
        "                if pmid in pubmed_results:\n",
        "                    pub_details = pubmed_results[pmid].copy()\n",
        "\n",
        "                    # Check if it caused a classification change\n",
        "                    if str(pmid) in [str(p) for p in citation_pmids]:\n",
        "                        pub_details['caused_change'] = True\n",
        "                        citation_publications.append(pub_details)\n",
        "                    else:\n",
        "                        pub_details['caused_change'] = False\n",
        "                        other_publications.append(pub_details)\n",
        "\n",
        "                    publications.append(pub_details)\n",
        "\n",
        "            # Add citation PMIDs not in LitVar\n",
        "            for pmid in citation_pmids:\n",
        "                pmid_str = str(pmid)\n",
        "                if pmid_str not in litvar_pmids and pmid_str in pubmed_results:\n",
        "                    pub_details = pubmed_results[pmid_str].copy()\n",
        "                    pub_details['caused_change'] = True\n",
        "                    pub_details['source'] = 'ClinVar_citation_only'\n",
        "                    citation_publications.append(pub_details)\n",
        "                    publications.append(pub_details)\n",
        "\n",
        "            all_variant_data[variant_id] = {\n",
        "                'variant_id': variant_id,\n",
        "                'rsid': rsid,\n",
        "                'change_date': row['date_new'],\n",
        "                'date_old': row['date_old'],\n",
        "                'old_class': row['ClinicalSignificance_old_norm'],\n",
        "                'new_class': row['ClinicalSignificance_new_norm'],\n",
        "                'citation_pmids_from_json': citation_pmids,\n",
        "                'publications': publications,\n",
        "                'citation_publications': citation_publications,\n",
        "                'other_publications': other_publications,\n",
        "                'total_publications': len(publications),\n",
        "                'total_litvar_pmids': len(litvar_pmids),\n",
        "                'citations_matched': len(citation_publications)\n",
        "            }\n",
        "\n",
        "            if (idx + 1) % 500 == 0:\n",
        "                print(f\"      Processed {idx + 1}/{len(valid_variants)} variants...\")\n",
        "\n",
        "        stats = {\n",
        "            'total_variants': len(valid_variants),\n",
        "            'variants_with_pmids': sum(1 for v in all_variant_data.values() if v['total_litvar_pmids'] > 0),\n",
        "            'perfect_matches': sum(1 for v in all_variant_data.values()\n",
        "                                 if v['citation_pmids_from_json'] and\n",
        "                                 v['citations_matched'] == len(v['citation_pmids_from_json'])),\n",
        "            'partial_matches': sum(1 for v in all_variant_data.values()\n",
        "                                 if v['citation_pmids_from_json'] and\n",
        "                                 0 < v['citations_matched'] < len(v['citation_pmids_from_json'])),\n",
        "            'no_matches': sum(1 for v in all_variant_data.values()\n",
        "                            if v['citation_pmids_from_json'] and v['citations_matched'] == 0)\n",
        "        }\n",
        "\n",
        "        print(f\"\\n   PROCESSING SUMMARY:\")\n",
        "        print(f\"      Total variants: {stats['total_variants']}\")\n",
        "        print(f\"      Variants with LitVar PMIDs: {stats['variants_with_pmids']}\")\n",
        "        print(f\"      Perfect citation matches: {stats['perfect_matches']}\")\n",
        "        print(f\"      Partial citation matches: {stats['partial_matches']}\")\n",
        "        print(f\"      No citation matches: {stats['no_matches']}\")\n",
        "\n",
        "        print(f\"\\n   Completed processing {len(all_variant_data)} variants\")\n",
        "        self.all_variant_data = all_variant_data\n",
        "        return all_variant_data\n",
        "        \"\"\"\n",
        "        Process all variants and fetch their publication data.\n",
        "        Enhanced to handle cases where citation PMIDs aren't in LitVar results.\n",
        "        \"\"\"\n",
        "        print(f\"\\n FETCHING PUBLICATIONS FOR ALL VARIANTS\")\n",
        "\n",
        "        all_variant_data = {}\n",
        "\n",
        "        # Group by VariationID to combine citation PMIDs\n",
        "        grouped = changes_df.groupby('VariationID').agg({\n",
        "            'RS# (dbSNP)_new': 'first',\n",
        "            'date_new': 'max',\n",
        "            'date_old': 'min',\n",
        "            'ClinicalSignificance_old_norm': 'first',\n",
        "            'ClinicalSignificance_new_norm': 'last',\n",
        "            'citation_pmids': lambda x: [pmid for sublist in x for pmid in sublist]\n",
        "        }).reset_index()\n",
        "\n",
        "        print(f\"   Found {len(grouped)} unique variants to process\")\n",
        "\n",
        "        # Track statistics\n",
        "        stats = {\n",
        "            'total_variants': len(grouped),\n",
        "            'variants_with_pmids': 0,\n",
        "            'perfect_matches': 0,\n",
        "            'partial_matches': 0,\n",
        "            'no_matches': 0\n",
        "        }\n",
        "\n",
        "        for idx, row in grouped.iterrows():\n",
        "            variant_id = str(row['VariationID'])\n",
        "            rsid = str(row['RS# (dbSNP)_new'])\n",
        "\n",
        "            if rsid and rsid != 'nan' and rsid != 'None':\n",
        "                print(f\"\\n   Processing variant {variant_id} (rs{rsid})\")\n",
        "\n",
        "                # Show citation PMIDs from JSON\n",
        "                citation_pmids = row['citation_pmids']\n",
        "                if citation_pmids:\n",
        "                    print(f\"      Citation PMIDs from JSON: {citation_pmids[:5]}...\" if len(citation_pmids) > 5 else f\"      Citation PMIDs from JSON: {citation_pmids}\")\n",
        "\n",
        "                # Fetch from LitVar\n",
        "                pub_data = self.fetch_litvar_publications_with_details(rsid, citation_pmids)\n",
        "\n",
        "                # If no matches found, also fetch the citation PMIDs directly from PubMed\n",
        "                if citation_pmids and len(pub_data['citation_publications']) == 0:\n",
        "                    print(f\"      Fetching citation PMIDs directly from PubMed...\")\n",
        "                    for pmid in citation_pmids:\n",
        "                        clean_pmid = str(pmid).strip()\n",
        "                        pub_details = self.fetch_publication_details(clean_pmid)\n",
        "                        if pub_details and (pub_details.get('journal') or pub_details.get('title')):\n",
        "                            pub_details['caused_change'] = True\n",
        "                            pub_details['source'] = 'ClinVar_citation'\n",
        "                            pub_data['citation_publications'].append(pub_details)\n",
        "                            pub_data['publications'].append(pub_details)\n",
        "                            print(f\"         Added citation PMID {clean_pmid} from PubMed\")\n",
        "\n",
        "                # Update statistics\n",
        "                if pub_data['pmids']:\n",
        "                    stats['variants_with_pmids'] += 1\n",
        "\n",
        "                if citation_pmids:\n",
        "                    if len(pub_data['citation_publications']) == len(citation_pmids):\n",
        "                        stats['perfect_matches'] += 1\n",
        "                    elif len(pub_data['citation_publications']) > 0:\n",
        "                        stats['partial_matches'] += 1\n",
        "                    else:\n",
        "                        stats['no_matches'] += 1\n",
        "\n",
        "                all_variant_data[variant_id] = {\n",
        "                    'variant_id': variant_id,\n",
        "                    'rsid': rsid,\n",
        "                    'change_date': row['date_new'],\n",
        "                    'date_old': row['date_old'],\n",
        "                    'old_class': row['ClinicalSignificance_old_norm'],\n",
        "                    'new_class': row['ClinicalSignificance_new_norm'],\n",
        "                    'citation_pmids_from_json': citation_pmids,\n",
        "                    'publications': pub_data['publications'],\n",
        "                    'citation_publications': pub_data['citation_publications'],\n",
        "                    'other_publications': pub_data['other_publications'],\n",
        "                    'total_publications': len(pub_data['publications']),\n",
        "                    'total_litvar_pmids': len(pub_data.get('pmids', [])),\n",
        "                    'citations_matched': len(pub_data['citation_publications'])\n",
        "                }\n",
        "\n",
        "        print(f\"\\n   PROCESSING SUMMARY:\")\n",
        "        print(f\"      Total variants: {stats['total_variants']}\")\n",
        "        print(f\"      Variants with LitVar PMIDs: {stats['variants_with_pmids']}\")\n",
        "        print(f\"      Perfect citation matches: {stats['perfect_matches']}\")\n",
        "        print(f\"      Partial citation matches: {stats['partial_matches']}\")\n",
        "        print(f\"      No citation matches: {stats['no_matches']}\")\n",
        "\n",
        "        self.all_variant_data = all_variant_data\n",
        "        return all_variant_data\n",
        "\n",
        "    def calculate_variant_representativeness(self, variant_data: Dict) -> float:\n",
        "        \"\"\"\n",
        "        Calculate how representative a variant is for visualization.\n",
        "        \"\"\"\n",
        "        score = 0\n",
        "\n",
        "        # Citations that caused changes, HIGHEST PRIORITY\n",
        "        score += len(variant_data['citation_publications']) * 25\n",
        "\n",
        "        # Number of total publications\n",
        "        score += min(variant_data['total_publications'], 50) * 2\n",
        "\n",
        "        # Classification change significance\n",
        "        change_sig = self.calculate_change_significance(\n",
        "            variant_data['old_class'],\n",
        "            variant_data['new_class']\n",
        "        )\n",
        "        score += change_sig * 15\n",
        "\n",
        "        # Publication diversity\n",
        "        if variant_data['publications']:\n",
        "            journals = set(p['journal'] for p in variant_data['publications'] if p.get('journal'))\n",
        "            score += len(journals) * 3\n",
        "\n",
        "        # High impact publications\n",
        "        for pub in variant_data['publications']:\n",
        "            if pub.get('impact_factor', 0) > 10:\n",
        "                score += 5\n",
        "            elif pub.get('impact_factor', 0) > 5:\n",
        "                score += 2\n",
        "\n",
        "        # If have both LitVar and citation data\n",
        "        if variant_data.get('total_litvar_pmids', 0) > 0 and variant_data.get('citations_matched', 0) > 0:\n",
        "            score += 20\n",
        "\n",
        "        return score\n",
        "\n",
        "    def calculate_change_significance(self, old_class: str, new_class: str) -> float:\n",
        "        \"\"\"\n",
        "        Calculate significance score for classification change.\n",
        "        \"\"\"\n",
        "        significance_map = {\n",
        "            'Benign': 0,\n",
        "            'Likely benign': 1,\n",
        "            'Uncertain significance': 2,\n",
        "            'Conflicting interpretations': 2.5,\n",
        "            'Likely pathogenic': 3,\n",
        "            'Pathogenic': 4,\n",
        "            'Unknown': 2\n",
        "        }\n",
        "\n",
        "        old_score = significance_map.get(old_class, 2)\n",
        "        new_score = significance_map.get(new_class, 2)\n",
        "\n",
        "        return abs(new_score - old_score)\n",
        "\n",
        "    def select_top_variants(self, all_variant_data: Dict, top_n: int = 10) -> Dict:\n",
        "        \"\"\"\n",
        "        Select top N most representative variants.\n",
        "        \"\"\"\n",
        "        print(f\"\\n SELECTING TOP {top_n} REPRESENTATIVE VARIANTS\")\n",
        "\n",
        "        variant_scores = []\n",
        "\n",
        "        for variant_id, data in all_variant_data.items():\n",
        "            score = self.calculate_variant_representativeness(data)\n",
        "            variant_scores.append({\n",
        "                'variant_id': variant_id,\n",
        "                'score': score,\n",
        "                'data': data\n",
        "            })\n",
        "\n",
        "        # Sort by score\n",
        "        variant_scores.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "        top_variants = {}\n",
        "        for item in variant_scores[:top_n]:\n",
        "            top_variants[item['variant_id']] = item['data']\n",
        "            print(f\"   Selected: Variant {item['variant_id']} (Score: {item['score']:.1f})\")\n",
        "            print(f\"      Change: {item['data']['old_class']} → {item['data']['new_class']}\")\n",
        "            print(f\"      Publications: {item['data']['total_publications']}\")\n",
        "            print(f\"      Citations causing change: {len(item['data']['citation_publications'])}\")\n",
        "\n",
        "        return top_variants\n",
        "\n",
        "    def create_timeline_scatter_visualization(self, top_variants: Dict) -> go.Figure:\n",
        "        \"\"\"\n",
        "        Create timeline scatter plot with journal CiteScores.\n",
        "        Y-axis: Journal CiteScore, LOG SCALED\n",
        "        X-axis: Publication date\n",
        "        Red dots for publications that caused classification changes\n",
        "        Blue dots for other publications\n",
        "        \"\"\"\n",
        "        print(f\"\\n CREATING TIMELINE SCATTER VISUALIZATION\")\n",
        "\n",
        "        n_variants = len(top_variants)\n",
        "\n",
        "        vertical_spacing = 0.02\n",
        "\n",
        "        fig = make_subplots(\n",
        "            rows=n_variants,\n",
        "            cols=1,\n",
        "            subplot_titles=[\n",
        "                f\"Variant {vid} (rs{data['rsid']}): {data['old_class']} → {data['new_class']}\"\n",
        "                for vid, data in top_variants.items()\n",
        "            ],\n",
        "            vertical_spacing=vertical_spacing,\n",
        "            row_heights=[1/n_variants] * n_variants\n",
        "        )\n",
        "\n",
        "        for idx, (variant_id, data) in enumerate(top_variants.items(), 1):\n",
        "            citation_pubs = []  # Publications that caused changes\n",
        "            other_pubs = []     # Other publications\n",
        "\n",
        "            for pub in data['publications']:\n",
        "                if pub['publication_date'] is not None:\n",
        "                    if pub.get('impact_factor', 2.0) <= 0:\n",
        "                        pub['impact_factor'] = 1.0\n",
        "\n",
        "                    if pub.get('caused_change', False):\n",
        "                        citation_pubs.append(pub)\n",
        "                    else:\n",
        "                        other_pubs.append(pub)\n",
        "\n",
        "            # Plot other publications with blue dots\n",
        "            if other_pubs:\n",
        "                dates = [p['publication_date'] for p in other_pubs]\n",
        "                impacts = [float(p.get('impact_factor', 2.0)) for p in other_pubs]\n",
        "\n",
        "                # Create hover text\n",
        "                labels = []\n",
        "                for p in other_pubs:\n",
        "                    original_score = float(p.get('impact_factor', 2.0))\n",
        "                    label = (\n",
        "                        f\"PMID: {p['pmid']}<br>\"\n",
        "                        f\"Journal: {p.get('journal', 'Unknown')[:50]}<br>\"\n",
        "                        f\"CiteScore: {original_score:.1f}<br>\"\n",
        "                        f\"Title: {p.get('title', '')[:100]}...\"\n",
        "                    )\n",
        "                    labels.append(label)\n",
        "\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(\n",
        "                        x=dates,\n",
        "                        y=impacts,\n",
        "                        mode='markers',\n",
        "                        name='Related Publications',\n",
        "                        marker=dict(\n",
        "                            size=8,\n",
        "                            color='blue',\n",
        "                            opacity=0.6,\n",
        "                            line=dict(width=1, color='darkblue')\n",
        "                        ),\n",
        "                        text=labels,\n",
        "                        hovertemplate='%{text}<br>Date: %{x|%Y-%m}<extra></extra>',\n",
        "                        showlegend=(idx == 1)\n",
        "                    ),\n",
        "                    row=idx, col=1\n",
        "                )\n",
        "\n",
        "            # Plot citation publications that caused changes with red dots\n",
        "            if citation_pubs:\n",
        "                dates = [p['publication_date'] for p in citation_pubs]\n",
        "                impacts = [float(p.get('impact_factor', 2.0)) for p in citation_pubs]\n",
        "\n",
        "                labels = []\n",
        "                for p in citation_pubs:\n",
        "                    original_score = float(p.get('impact_factor', 2.0))\n",
        "                    label = (\n",
        "                        f\"CAUSED CLASSIFICATION CHANGE <br>\"\n",
        "                        f\"PMID: {p['pmid']}<br>\"\n",
        "                        f\"Journal: {p.get('journal', 'Unknown')[:50]}<br>\"\n",
        "                        f\"CiteScore: {original_score:.1f}<br>\"\n",
        "                        f\"Title: {p.get('title', '')[:100]}...\"\n",
        "                    )\n",
        "                    labels.append(label)\n",
        "\n",
        "                fig.add_trace(\n",
        "                    go.Scatter(\n",
        "                        x=dates,\n",
        "                        y=impacts,\n",
        "                        mode='markers',\n",
        "                        name='Publications Causing Change',\n",
        "                        marker=dict(\n",
        "                            size=10,\n",
        "                            color='red',\n",
        "                            opacity=0.9,\n",
        "                            line=dict(width=2, color='darkred')\n",
        "                        ),\n",
        "                        text=labels,\n",
        "                        hovertemplate='%{text}<br>Date: %{x|%Y-%m}<extra></extra>',\n",
        "                        showlegend=(idx == 1)\n",
        "                    ),\n",
        "                    row=idx, col=1\n",
        "                )\n",
        "\n",
        "            # Add vertical line for classification change date\n",
        "            if data.get('change_date') and pd.notna(data['change_date']):\n",
        "                change_date = pd.to_datetime(data['change_date'])\n",
        "                if pd.notna(change_date):\n",
        "                    fig.add_shape(\n",
        "                        type=\"line\",\n",
        "                        x0=change_date, x1=change_date,\n",
        "                        y0=0.5, y1=1000,\n",
        "                        yref=f\"y{idx}\" if idx > 1 else \"y\",\n",
        "                        line=dict(\n",
        "                            color=\"red\",\n",
        "                            width=2,\n",
        "                            dash=\"dash\"\n",
        "                        ),\n",
        "                        opacity=0.4,\n",
        "                        row=idx, col=1\n",
        "                    )\n",
        "                    fig.add_annotation(\n",
        "                        x=change_date,\n",
        "                        y=1000,\n",
        "                        yref=f\"y{idx}\" if idx > 1 else \"y\",\n",
        "                        text=\"Changed\",\n",
        "                        showarrow=False,\n",
        "                        font=dict(size=9, color=\"red\"),\n",
        "                        yshift=10,\n",
        "                        row=idx, col=1\n",
        "                    )\n",
        "\n",
        "            # Add vertical line for previous classification date\n",
        "            if data.get('date_old') and pd.notna(data['date_old']):\n",
        "                old_date = pd.to_datetime(data['date_old'])\n",
        "                if pd.notna(old_date):\n",
        "                    fig.add_shape(\n",
        "                        type=\"line\",\n",
        "                        x0=old_date, x1=old_date,\n",
        "                        y0=0.5, y1=1000,\n",
        "                        yref=f\"y{idx}\" if idx > 1 else \"y\",\n",
        "                        line=dict(\n",
        "                            color=\"orange\",\n",
        "                            width=2,\n",
        "                            dash=\"dot\"\n",
        "                        ),\n",
        "                        opacity=0.3,\n",
        "                        row=idx, col=1\n",
        "                    )\n",
        "                    fig.add_annotation(\n",
        "                        x=old_date,\n",
        "                        y=1000,\n",
        "                        yref=f\"y{idx}\" if idx > 1 else \"y\",\n",
        "                        text=\"Previous\",\n",
        "                        showarrow=False,\n",
        "                        font=dict(size=9, color=\"orange\"),\n",
        "                        yshift=10,\n",
        "                        row=idx, col=1\n",
        "                    )\n",
        "            fig.update_yaxes(\n",
        "                title_text=\"CiteScore (log scale)\",\n",
        "                title_font_size=10,\n",
        "                type=\"log\",\n",
        "                row=idx, col=1,\n",
        "                range=[np.log10(0.5), np.log10(1000)],\n",
        "                tickvals=[0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000],\n",
        "                ticktext=['0.5', '1', '2', '5', '10', '20', '50', '100', '200', '500', '1000'],  # Labels\n",
        "                tickfont=dict(size=9),\n",
        "                showgrid=True,\n",
        "                gridwidth=0.5,\n",
        "                gridcolor='lightgray',\n",
        "                minor=dict(showgrid=True, gridcolor='#f0f0f0', gridwidth=0.3)  # Minor gridlines\n",
        "            )\n",
        "\n",
        "            fig.update_xaxes(\n",
        "                tickfont=dict(size=9),\n",
        "                row=idx, col=1,\n",
        "                showgrid=True,\n",
        "                gridwidth=0.5,\n",
        "                gridcolor='lightgray'\n",
        "            )\n",
        "\n",
        "        fig.update_layout(\n",
        "            height=500 * n_variants,\n",
        "            title_text=\"Publication Timeline with Journal CiteScores (Log Scale)<br>\" +\n",
        "                    \"<sub>Red: citations that caused classification change | Blue: other related publications | \" +\n",
        "                    \"Dashed lines: classification change dates</sub>\",\n",
        "            title_font_size=14,\n",
        "            hovermode='closest',\n",
        "            showlegend=True,\n",
        "            legend=dict(\n",
        "                orientation=\"h\",\n",
        "                yanchor=\"top\",\n",
        "                y=1.003,\n",
        "                xanchor=\"center\",\n",
        "                x=0.5,\n",
        "                font=dict(size=10),\n",
        "                bgcolor=\"rgba(255,255,255,0.8)\"\n",
        "            ),\n",
        "            margin=dict(t=60, b=30, l=60, r=30),\n",
        "            plot_bgcolor='white',\n",
        "            paper_bgcolor='white'\n",
        "        )\n",
        "\n",
        "        fig.update_xaxes(\n",
        "            title_text=\"Publication Date\",\n",
        "            title_font_size=11,\n",
        "            row=n_variants, col=1\n",
        "        )\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def generate_summary_report(self, all_variant_data: Dict, top_variants: Dict, output_dir: str):\n",
        "        \"\"\"\n",
        "        Generate comprehensive summary report.\n",
        "        \"\"\"\n",
        "        report = []\n",
        "        report.append(\"=\" * 80)\n",
        "        report.append(\"LITVAR ANALYSIS REPORT - JOURNAL IMPACT FACTOR TIMELINE\")\n",
        "        report.append(\"=\" * 80)\n",
        "        report.append(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        report.append(\"\")\n",
        "\n",
        "        total_variants = len(all_variant_data)\n",
        "        total_pubs = sum(d['total_publications'] for d in all_variant_data.values())\n",
        "        total_citations = sum(len(d['citation_publications']) for d in all_variant_data.values())\n",
        "\n",
        "        report.append(\"OVERALL STATISTICS:\")\n",
        "        report.append(f\"  Total variants analyzed: {total_variants}\")\n",
        "        report.append(f\"  Total publications found: {total_pubs}\")\n",
        "        report.append(f\"  Publications causing classification changes: {total_citations}\")\n",
        "        report.append(\"\")\n",
        "\n",
        "        report.append(\"TOP 10 REPRESENTATIVE VARIANTS:\")\n",
        "        for i, (variant_id, data) in enumerate(top_variants.items(), 1):\n",
        "            report.append(f\"\\n  {i}. Variant {variant_id} (rs{data['rsid']}):\")\n",
        "            report.append(f\"     Classification: {data['old_class']} → {data['new_class']}\")\n",
        "            report.append(f\"     Total publications: {data['total_publications']}\")\n",
        "            report.append(f\"     Publications causing change: {len(data['citation_publications'])}\")\n",
        "\n",
        "            if data['citation_publications']:\n",
        "                report.append(\"     Key publications causing change:\")\n",
        "                for pub in data['citation_publications'][:3]:\n",
        "                    report.append(f\"       - PMID {pub['pmid']}: {pub['journal']} (IF: {pub['impact_factor']:.1f})\")\n",
        "\n",
        "        report.append(\"\")\n",
        "        report.append(\"JOURNAL IMPACT FACTOR DISTRIBUTION:\")\n",
        "        all_impacts = []\n",
        "        for data in all_variant_data.values():\n",
        "            all_impacts.extend([p['impact_factor'] for p in data['publications']])\n",
        "\n",
        "        if all_impacts:\n",
        "            report.append(f\"  Average impact factor: {np.mean(all_impacts):.2f}\")\n",
        "            report.append(f\"  Median impact factor: {np.median(all_impacts):.2f}\")\n",
        "            report.append(f\"  Max impact factor: {np.max(all_impacts):.2f}\")\n",
        "\n",
        "        report_text = \"\\n\".join(report)\n",
        "\n",
        "        with open(f\"{output_dir}/analysis_report.txt\", 'w') as f:\n",
        "            f.write(report_text)\n",
        "\n",
        "        print(\"\\n\" + report_text)\n",
        "\n",
        "    def run_complete_analysis(self, json_file: str, output_dir: str = './litvar_results'):\n",
        "        \"\"\"\n",
        "        Run complete analysis pipeline with JSON input\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"STARTING LITVAR ANALYSIS WITH JSON INPUT\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Load classification changes from JSON\n",
        "        changes_df = self.load_json_classification_changes(json_file)\n",
        "\n",
        "        if changes_df.empty:\n",
        "            print(\"No classification changes found in JSON\")\n",
        "            return None\n",
        "\n",
        "        changes_df.to_csv(f\"{output_dir}/processed_changes.csv\", index=False)\n",
        "\n",
        "        # Process all variants using batch processing\n",
        "        all_variant_data = self.process_all_variants_optimized(changes_df)\n",
        "\n",
        "        # Select top 10 representative variants\n",
        "        top_variants = self.select_top_variants(all_variant_data, top_n=10)\n",
        "\n",
        "        # Create timeline scatter visualization\n",
        "        timeline_fig = self.create_timeline_scatter_visualization(top_variants)\n",
        "        timeline_fig.write_html(f\"{output_dir}/timeline_scatter_impact_factors.html\")\n",
        "        print(f\"   Saved timeline scatter plot visualization\")\n",
        "\n",
        "        # Generate ML features\n",
        "        ml_features = self.generate_ml_features(all_variant_data)\n",
        "        ml_features.to_csv(f\"{output_dir}/ml_features.csv\", index=False)\n",
        "        print(f\"   Saved ML features for {len(ml_features)} variants\")\n",
        "\n",
        "        # Generate summary report\n",
        "        self.generate_summary_report(all_variant_data, top_variants, output_dir)\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"ANALYSIS COMPLETE\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Total runtime: {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)\")\n",
        "        print(f\"Results saved to: {output_dir}\")\n",
        "\n",
        "\n",
        "    def generate_ml_features(self, all_variant_data: Dict) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Generate ML features for model training.\n",
        "        \"\"\"\n",
        "        print(f\"\\n GENERATING ML FEATURES\")\n",
        "\n",
        "        features_list = []\n",
        "\n",
        "        for variant_id, data in all_variant_data.items():\n",
        "            feature_dict = {\n",
        "                'variant_id': variant_id,\n",
        "                'rsid': data['rsid'],\n",
        "                'old_class': data['old_class'],\n",
        "                'new_class': data['new_class'],\n",
        "                'total_publications': data['total_publications'],\n",
        "                'citations_causing_change': len(data['citation_publications']),\n",
        "                'change_significance': self.calculate_change_significance(\n",
        "                    data['old_class'], data['new_class']\n",
        "                )\n",
        "            }\n",
        "\n",
        "            # Publication temporal features\n",
        "            if data['publications']:\n",
        "                valid_pubs = [p for p in data['publications'] if p['publication_date']]\n",
        "                if valid_pubs and data['change_date']:\n",
        "                    # Publications before and after change\n",
        "                    before = sum(1 for p in valid_pubs if p['publication_date'] < data['change_date'])\n",
        "                    after = sum(1 for p in valid_pubs if p['publication_date'] >= data['change_date'])\n",
        "                    feature_dict['pubs_before_change'] = before\n",
        "                    feature_dict['pubs_after_change'] = after\n",
        "\n",
        "                    # Recent publication surge (6 months before change)\n",
        "                    six_months_before = data['change_date'] - pd.Timedelta(days=180)\n",
        "                    recent_pubs = sum(1 for p in valid_pubs\n",
        "                                    if six_months_before <= p['publication_date'] < data['change_date'])\n",
        "                    feature_dict['recent_publication_surge'] = recent_pubs\n",
        "\n",
        "            # Impact factor features\n",
        "            if data['publications']:\n",
        "                impacts = [p['impact_factor'] for p in data['publications']]\n",
        "                feature_dict['mean_impact_factor'] = np.mean(impacts) if impacts else 0\n",
        "                feature_dict['max_impact_factor'] = np.max(impacts) if impacts else 0\n",
        "                feature_dict['high_impact_count'] = sum(1 for i in impacts if i > 10)\n",
        "\n",
        "            # Journal diversity\n",
        "            if data['publications']:\n",
        "                journals = set(p['journal'] for p in data['publications'] if p['journal'])\n",
        "                feature_dict['unique_journals'] = len(journals)\n",
        "\n",
        "            features_list.append(feature_dict)\n",
        "\n",
        "        features_df = pd.DataFrame(features_list)\n",
        "        features_df = features_df.fillna(0)\n",
        "\n",
        "        print(f\"   Generated {len(features_df)} feature rows with {len(features_df.columns)} features\")\n",
        "\n",
        "        return features_df\n",
        "\n",
        "\n",
        "def main():\n",
        "    EMAIL = \"yl8889@nyu.edu\"\n",
        "    API_KEY = \"\"\n",
        "    JSON_FILE = \"/content/BRCA1_changes_parallel_20250815_234141.json\"\n",
        "    IMPACT_FACTOR_CSV = \"/content/journal_ranking_data.csv\"\n",
        "    OUTPUT_DIR = \"./litvar_analysis_results\"\n",
        "\n",
        "    print(\"Initializing Enhanced LitVar Analyzer...\")\n",
        "    analyzer = EnhancedLitVarAnalyzer(\n",
        "        email=EMAIL,\n",
        "        api_key=API_KEY,\n",
        "        impact_factor_csv=IMPACT_FACTOR_CSV\n",
        "    )\n",
        "\n",
        "    results = analyzer.run_complete_analysis(JSON_FILE, OUTPUT_DIR)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8FeC70MYV9G"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}